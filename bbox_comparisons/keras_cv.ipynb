{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\n",
    "    # if isinstance(value, str):\n",
    "        # value = value.encode('utf-8')\n",
    "    \n",
    "    # value = tf.io.serialize_tensor(value.numpy())\n",
    "    print(value)\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _convert_to_example(image_example, image_buffer, height, width):\n",
    "    \"\"\"Build an Example proto for an example.\n",
    "    Args:\n",
    "      image_example: dict, an image example\n",
    "      image_buffer: string, JPEG encoding of RGB image\n",
    "      height: integer, image height in pixels\n",
    "      width: integer, image width in pixels\n",
    "    Returns:\n",
    "      Example proto\n",
    "    \"\"\"\n",
    "\n",
    "    # Required\n",
    "    filename = str(image_example['filename'])\n",
    "    id = str(image_example['id'])\n",
    "\n",
    "    # Class label for the whole image\n",
    "    image_class = image_example.get('class', {})\n",
    "    class_label = image_class.get('label', 0)\n",
    "    class_text = str(image_class.get('text', b''))\n",
    "\n",
    "    # Bounding Boxes\n",
    "    image_objects = image_example.get('object', {})\n",
    "    image_bboxes = image_objects.get('bbox', {})\n",
    "    xmin = image_bboxes.get('xmin', [])\n",
    "    xmax = image_bboxes.get('xmax', [])\n",
    "    ymin = image_bboxes.get('ymin', [])\n",
    "    ymax = image_bboxes.get('ymax', [])\n",
    "    bbox_labels = image_bboxes.get('label', [])\n",
    "    bbox_scores = image_bboxes.get('score', [])\n",
    "    bbox_count = image_bboxes.get('count', 0)\n",
    "\n",
    "    # Parts\n",
    "    image_parts = image_objects.get('parts', {})\n",
    "    parts_x = image_parts.get('x', [])\n",
    "    parts_y = image_parts.get('y', [])\n",
    "    parts_v = image_parts.get('v', [])\n",
    "\n",
    "    # Areas\n",
    "    object_areas = image_objects.get('area', [])\n",
    "\n",
    "    # Ids\n",
    "    object_ids = image_objects.get('id', [])\n",
    "\n",
    "    colorspace = b'RGB'\n",
    "    channels = 3\n",
    "    image_format = b'JPEG'\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': _int64_feature(height),\n",
    "        'image/width': _int64_feature(width),\n",
    "        'image/colorspace': _bytes_feature(colorspace),\n",
    "        'image/channels': _int64_feature(channels),\n",
    "        'image/class/label': _int64_feature(class_label),\n",
    "        'image/class/text': _bytes_feature(class_text),\n",
    "        'image/object/bbox/xmin': _float_feature(xmin),\n",
    "        'image/object/bbox/xmax': _float_feature(xmax),\n",
    "        'image/object/bbox/ymin': _float_feature(ymin),\n",
    "        'image/object/bbox/ymax': _float_feature(ymax),\n",
    "        'image/object/bbox/xyxy': _float_feature([xmin,ymin,xmax,ymax]),\n",
    "        'image/object/bbox/label': _int64_feature(bbox_labels),\n",
    "        'image/object/bbox/count': _int64_feature(bbox_count),\n",
    "        'image/object/bbox/score': _float_feature(bbox_scores),\n",
    "        'image/object/parts/x': _float_feature(parts_x),\n",
    "        'image/object/parts/y': _float_feature(parts_y),\n",
    "        'image/object/parts/v': _int64_feature(parts_v),\n",
    "        'image/object/parts/count': _int64_feature(len(parts_v)),\n",
    "        'image/object/area': _float_feature(object_areas),\n",
    "        'image/object/id': _int64_feature(object_ids),\n",
    "        'image/format': _bytes_feature(image_format),\n",
    "        'image/filename': _bytes_feature(os.path.basename(filename)),\n",
    "        'image/path': _bytes_feature(os.path.dirname(filename)),\n",
    "        'image/id': _bytes_feature(str(id)),\n",
    "        'image/encoded': _bytes_feature(image_buffer)}))\n",
    "    return example\n",
    "\n",
    "def _process_image(filename):\n",
    "    \"\"\"Process a single image file.\n",
    "    Args:\n",
    "      filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
    "    Returns:\n",
    "      image: string, JPEG encoding of RGB image.\n",
    "      height: integer, image height in pixels.\n",
    "      width: integer, image width in pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    # bring in the image, convert to MxNx3 uint tensor\n",
    "    image = tf.io.decode_image(tf.io.read_file(filename), channels=3) \n",
    "    assert len(image.shape) == 3, 'Image must be 3 dimensions'\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    assert image.shape[2] == 3, 'Image must have 3 channels, RGB'\n",
    "\n",
    "    # convert it into a jpeg bytes-string -- pull the data out of the tensor!\n",
    "    image_bytes = tf.io.encode_jpeg(image, quality=100).numpy()\n",
    "\n",
    "    return image_bytes, height, width\n",
    "\n",
    "\n",
    "def _process_image_files_batch(thread_index, ranges, name, output_directory, dataset, num_shards, error_queue):\n",
    "    \"\"\"Processes and saves list of images as TFRecord in 1 thread.\n",
    "    Args:\n",
    "      thread_index: integer, unique batch to run index is within [0, len(ranges)).\n",
    "      ranges: list of pairs of integers specifying ranges of each batches to\n",
    "        analyze in parallel.\n",
    "      name: string, unique identifier specifying the data set (e.g. `train` or `test`)\n",
    "      output_directory: string, file path to store the tfrecord files.\n",
    "      dataset: list, a list of image example dicts\n",
    "      num_shards: integer number of shards for this data set.\n",
    "      error_queue: Queue, a queue to place image examples that failed.\n",
    "    \"\"\"\n",
    "    # Each thread produces N shards where N = int(num_shards / num_threads).\n",
    "    # For instance, if num_shards = 128, and the num_threads = 2, then the first\n",
    "    # thread would produce shards [0, 64).\n",
    "    num_threads = len(ranges)\n",
    "    assert not num_shards % num_threads\n",
    "    num_shards_per_batch = int(num_shards / num_threads)\n",
    "\n",
    "    shard_ranges = np.linspace(ranges[thread_index][0],\n",
    "                               ranges[thread_index][1],\n",
    "                               num_shards_per_batch + 1).astype(int)\n",
    "    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n",
    "\n",
    "    counter = 0\n",
    "    error_counter = 0\n",
    "    for s in range(num_shards_per_batch):\n",
    "        # Generate a sharded version of the file name, e.g. 'train-00002-of-00010'\n",
    "        shard = thread_index * num_shards_per_batch + s\n",
    "        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\n",
    "        output_file = os.path.join(output_directory, output_filename)\n",
    "        writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "        shard_counter = 0\n",
    "        files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n",
    "        for i in files_in_shard:\n",
    "\n",
    "            image_example = dataset[i]\n",
    "\n",
    "            filename = str(image_example['filename'])\n",
    "\n",
    "            try:\n",
    "                image_buffer, height, width = _process_image(filename)\n",
    "\n",
    "                example = _convert_to_example(image_example, image_buffer, height, width)\n",
    "                writer.write(example.SerializeToString())\n",
    "                shard_counter += 1\n",
    "                counter += 1\n",
    "            except Exception as e:\n",
    "                raise\n",
    "                error_counter += 1\n",
    "                error_queue.put(image_example)\n",
    "\n",
    "        shard_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resource'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\17204\\Documents\\git\\Work_Tools\\bbox_comparisons\\keras_cv.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/17204/Documents/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/17204/Documents/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras_core\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/17204/Documents/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/17204/Documents/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/17204/Documents/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image, ImageDraw\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras_cv\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m \u001b[39mimport\u001b[39;00m bounding_box\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m \u001b[39mimport\u001b[39;00m callbacks\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m \u001b[39mimport\u001b[39;00m losses\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras_cv\\datasets\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The KerasCV Authors\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m pascal_voc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras_cv\\datasets\\pascal_voc\\__init__.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The KerasCV Authors\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpascal_voc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m load\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras_cv\\datasets\\pascal_voc\\load.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The KerasCV Authors\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m \u001b[39mimport\u001b[39;00m bounding_box\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_export\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_cv_export\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m _TIMESTAMP_IMPORT_STARTS \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[1;32m---> 43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_tfds_logging\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m call_metadata \u001b[39mas\u001b[39;00m _call_metadata\n\u001b[0;32m     46\u001b[0m _metadata \u001b[39m=\u001b[39m _call_metadata\u001b[39m.\u001b[39mCallMetadata()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\__init__.py:22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Allow to use `tfds.core.Path` in dataset implementation which seems more\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# natural than having to import a third party module.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mepath\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m community\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m BeamBasedBuilder\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m BuilderConfig\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\community\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Copyright 2023 The TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"Community dataset API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhuggingface_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m mock_builtin_to_use_gfile\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhuggingface_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m mock_huggingface_import\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m builder_cls_from_module\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\community\\huggingface_wrapper.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munittest\u001b[39;00m \u001b[39mimport\u001b[39;00m mock\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m \u001b[39mimport\u001b[39;00m epath\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_builder\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_info\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m download\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m reader \u001b[39mas\u001b[39;00m reader_lib\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m registered\n\u001b[1;32m---> 44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m split_builder \u001b[39mas\u001b[39;00m split_builder_lib\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m splits \u001b[39mas\u001b[39;00m splits_lib\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_compat\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\split_builder.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m splits \u001b[39mas\u001b[39;00m splits_lib\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m writer \u001b[39mas\u001b[39;00m writer_lib\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m shard_utils\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m typing\u001b[39m.\u001b[39mTYPE_CHECKING:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\writer.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m lazy_imports_lib\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m naming\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m shuffle\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m file_utils\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\shuffle.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mresource\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstruct\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Iterator, List, Optional\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'resource'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_cv\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import glob\n",
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_func(record_bytes):\n",
    "    # Parse an Example to access the Features\n",
    "    example = tf.io.parse_single_example(\n",
    "      record_bytes,\n",
    "      features={\n",
    "        # 'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        # 'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        # 'image/colorspace': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'image/channels': tf.io.FixedLenFeature([], tf.int64),\n",
    "        # 'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        # 'image/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xyxy': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/label': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/bbox/count': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/bbox/score': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        # 'image/object/parts/x': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        # 'image/object/parts/y': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        # 'image/object/parts/v': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        # 'image/object/parts/count': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        # 'image/object/area': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        # 'image/object/id': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        # 'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'image/path': tf.io.FixedLenFeature([], tf.string),\n",
    "        # 'image/id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string)\n",
    "      }\n",
    "    )\n",
    "\n",
    "    bounding_boxes: {\n",
    "            'boxes': tf.sparse_to_dense(example['image/object/bbox/xyxy']),\n",
    "            'classes': tf.sparse_to_dense(example['image/object/bbox/label'])}\n",
    "    images = tf.io.decode_jpeg(tf.sparse_to_dense(example['image/encoded']), channels=3)\n",
    "\n",
    "    return (bounding_box, images)\n",
    "\n",
    "    # image = tf.io.parse_single_example(\n",
    "    #   record_bytes,\n",
    "    #   features={\n",
    "    #     'image/encoded': tf.io.FixedLenFeature([], tf.string)\n",
    "    #   }\n",
    "    # )\n",
    "\n",
    "    # bbox = tf.io.parse_single_example(\n",
    "    #   record_bytes,\n",
    "    #   features={\n",
    "    #     'image/object/bbox/xmin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    #     'image/object/bbox/ymin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    #     'image/object/bbox/xmax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    #     'image/object/bbox/ymax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    #     'image/object/bbox/label': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "    #     'image/object/bbox/count': tf.io.FixedLenFeature([], tf.int64),\n",
    "    #     'image/object/bbox/score': tf.io.VarLenFeature(dtype=tf.float32)\n",
    "    #   }\n",
    "    # )\n",
    "    \n",
    "    # return (image, bbox)\n",
    "\n",
    "def load_dataset(filenames):\n",
    "    # allow the dataset to come in whenever\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "\n",
    "    # map the function\n",
    "    dataset = dataset.map(decode_func, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # bbox_x0 = dataset['image/object/bbox/xmin']\n",
    "    # bbox_y0 = dataset['image/object/bbox/ymin']\n",
    "    # bbox_x1 = dataset['image/object/bbox/xmax']\n",
    "    # bbox_y1 = dataset['image/object/bbox/ymax']\n",
    "\n",
    "    # # create a tensor array of [xmin, ymin, xmax, ymax] for each bbox\n",
    "    # bbox = tf.concat((bbox_x0, bbox_x1, bbox_y0, bbox_y1), axis=-1)\n",
    "    # bbox = tf.transpose(bbox)\n",
    "\n",
    "    # # pull out the image data\n",
    "    # image = dataset['image/encoded']\n",
    "\n",
    "    # return (image,bbox)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_dataset(filenames):\n",
    "    dataset = load_dataset(filenames)\n",
    "    dataset = dataset.shuffle(1000,seed=42)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "    for n in range(4):\n",
    "        im = Image.fromarray(tf.image.decode_jpeg(image_batch[n]))\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        for bbox in label_batch[n]:\n",
    "            draw.rectangle(bbox)\n",
    "        im.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset\n",
    "\n",
    "# get the data\n",
    "os.chdir('/home/klb807/MARS_project/test/detection/black_top_tfrecords_detection')\n",
    "train_files = glob.glob('./train_dataset*')\n",
    "train_dataset = get_dataset(train_files)\n",
    "\n",
    "cur_epoch = next(iter(train_dataset))\n",
    "\n",
    "# print(tf.stack((cur_epoch['image/object/bbox/xmin'].values,\n",
    "#           cur_epoch['image/object/bbox/xmax'].values,\n",
    "#           cur_epoch['image/object/bbox/ymin'].values,\n",
    "#           cur_epoch['image/object/bbox/ymax'].values),\n",
    "#           axis=-1).numpy())\n",
    "\n",
    "\n",
    "# print(cur_epoch['image/object/bbox/xmin'].values)\n",
    "\n",
    "print(cur_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the model\n",
    "mdl = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=16,\n",
    "    bounding_box_format='xyxy',\n",
    "    backbone=keras_cv.models.YOLOV8Backbone.from_preset('yolo_v8_m_backbone_coco'),\n",
    "    fpn_depth=2\n",
    ")\n",
    "\n",
    "mdl.compile(\n",
    "    classification_loss='binary_crossentropy',\n",
    "    box_loss='ciou',\n",
    "    optimizer=tf.optimizers.Adamax(),\n",
    "    jit_compile=False\n",
    ")\n",
    "\n",
    "\n",
    "# get the list of tfrecord files\n",
    "os.chdir('/home/klb807/MARS_project/test/detection/black_top_tfrecords_detection')\n",
    "\n",
    "train_files = glob.glob('./train_dataset*')\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(train_files).map(decode_func)\n",
    "\n",
    "# train the model\n",
    "mdl.fit()\n",
    "\n",
    "# for image_fn in os.listdir('.'):\n",
    "# # for image_fn in ['013009_A25_Block6_castBCma1_t_4677.png']:\n",
    "#     if '.png' in image_fn:\n",
    "#         with open(image_fn,'rb') as fid:\n",
    "#             # bring in image data, create PIL image for later display\n",
    "#             image_data = tf.image.decode_image(fid.read(),channels=3)\n",
    "#             # image_data = tf.image.resize_with_crop_or_pad(image_data, 512, 512)\n",
    "#             image_data = tf.image.resize_with_pad(image_data, 512, 512)\n",
    "#             image_data = tf.expand_dims(image_data, axis=0)\n",
    "\n",
    "#             output = mdl.predict(image_data)\n",
    "            \n",
    "#             # display the image and the bounding boxes \n",
    "#             im = Image.fromarray(np.squeeze(image_data.numpy()).astype(np.uint8))\n",
    "#             draw = ImageDraw.Draw(im)\n",
    "#             for i_box,box in enumerate(output['boxes'][0]):\n",
    "#                 clrs = int(16*output['classes'][0][i_box])\n",
    "#                 draw.rectangle(box, outline=(255-clrs, clrs, clrs, 0))\n",
    "            \n",
    "#             im.show()\n",
    "#             i = input()\n",
    "\n",
    "#             if i.lower() == 'stop':\n",
    "#                 break\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/klb807/MARS_project/test/detection/black_top_tfrecords_detection')\n",
    "\n",
    "train_dataset = glob.glob('./train_dataset*')\n",
    "\n",
    "# for batch in tf.data.TFRecordDataset(train_dataset).map(decode_fn).take(10):\n",
    "#     print(batch)\n",
    "\n",
    "for item in tf.data.TFRecordDataset(train_dataset).map(decode_func).take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item['image/object/bbox/xmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython import display\n",
    "\n",
    "# im = Image.fromarray(image_data)\n",
    "# im.\n",
    "# im.show()\n",
    "\n",
    "im = Image.fromarray(np.squeeze(image_data.numpy()).astype(np.uint8))\n",
    "im.show()\n",
    "# draw = ImageDraw.Draw(im)\n",
    "# output['boxes'][0][0]\n",
    "# im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openCV_tester",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
