{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\n",
    "    # if isinstance(value, str):\n",
    "        # value = value.encode('utf-8')\n",
    "    \n",
    "    # value = tf.io.serialize_tensor(value.numpy())\n",
    "    print(value)\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _convert_to_example(image_example, image_buffer, height, width):\n",
    "    \"\"\"Build an Example proto for an example.\n",
    "    Args:\n",
    "      image_example: dict, an image example\n",
    "      image_buffer: string, JPEG encoding of RGB image\n",
    "      height: integer, image height in pixels\n",
    "      width: integer, image width in pixels\n",
    "    Returns:\n",
    "      Example proto\n",
    "    \"\"\"\n",
    "\n",
    "    # Required\n",
    "    filename = str(image_example['filename'])\n",
    "    id = str(image_example['id'])\n",
    "\n",
    "    # Class label for the whole image\n",
    "    image_class = image_example.get('class', {})\n",
    "    class_label = image_class.get('label', 0)\n",
    "    class_text = str(image_class.get('text', b''))\n",
    "\n",
    "    # Bounding Boxes\n",
    "    image_objects = image_example.get('object', {})\n",
    "    image_bboxes = image_objects.get('bbox', {})\n",
    "    xmin = image_bboxes.get('xmin', [])\n",
    "    xmax = image_bboxes.get('xmax', [])\n",
    "    ymin = image_bboxes.get('ymin', [])\n",
    "    ymax = image_bboxes.get('ymax', [])\n",
    "    bbox_labels = image_bboxes.get('label', [])\n",
    "    bbox_scores = image_bboxes.get('score', [])\n",
    "    bbox_count = image_bboxes.get('count', 0)\n",
    "\n",
    "    # Parts\n",
    "    image_parts = image_objects.get('parts', {})\n",
    "    parts_x = image_parts.get('x', [])\n",
    "    parts_y = image_parts.get('y', [])\n",
    "    parts_v = image_parts.get('v', [])\n",
    "\n",
    "    # Areas\n",
    "    object_areas = image_objects.get('area', [])\n",
    "\n",
    "    # Ids\n",
    "    object_ids = image_objects.get('id', [])\n",
    "\n",
    "    colorspace = b'RGB'\n",
    "    channels = 3\n",
    "    image_format = b'JPEG'\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': _int64_feature(height),\n",
    "        'image/width': _int64_feature(width),\n",
    "        'image/colorspace': _bytes_feature(colorspace),\n",
    "        'image/channels': _int64_feature(channels),\n",
    "        'image/class/label': _int64_feature(class_label),\n",
    "        'image/class/text': _bytes_feature(class_text),\n",
    "        'image/object/bbox/xmin': _float_feature(xmin),\n",
    "        'image/object/bbox/xmax': _float_feature(xmax),\n",
    "        'image/object/bbox/ymin': _float_feature(ymin),\n",
    "        'image/object/bbox/ymax': _float_feature(ymax),\n",
    "        'image/object/bbox/label': _int64_feature(bbox_labels),\n",
    "        'image/object/bbox/count': _int64_feature(bbox_count),\n",
    "        'image/object/bbox/score': _float_feature(bbox_scores),\n",
    "        'image/object/parts/x': _float_feature(parts_x),\n",
    "        'image/object/parts/y': _float_feature(parts_y),\n",
    "        'image/object/parts/v': _int64_feature(parts_v),\n",
    "        'image/object/parts/count': _int64_feature(len(parts_v)),\n",
    "        'image/object/area': _float_feature(object_areas),\n",
    "        'image/object/id': _int64_feature(object_ids),\n",
    "        'image/format': _bytes_feature(image_format),\n",
    "        'image/filename': _bytes_feature(os.path.basename(filename)),\n",
    "        'image/path': _bytes_feature(os.path.dirname(filename)),\n",
    "        'image/id': _bytes_feature(str(id)),\n",
    "        'image/encoded': _bytes_feature(image_buffer)}))\n",
    "    return example\n",
    "\n",
    "def _process_image(filename):\n",
    "    \"\"\"Process a single image file.\n",
    "    Args:\n",
    "      filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
    "    Returns:\n",
    "      image: string, JPEG encoding of RGB image.\n",
    "      height: integer, image height in pixels.\n",
    "      width: integer, image width in pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    # bring in the image, convert to MxNx3 uint tensor\n",
    "    image = tf.io.decode_image(tf.io.read_file(filename), channels=3) \n",
    "    assert len(image.shape) == 3, 'Image must be 3 dimensions'\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    assert image.shape[2] == 3, 'Image must have 3 channels, RGB'\n",
    "\n",
    "    # convert it into a jpeg bytes-string -- pull the data out of the tensor!\n",
    "    image_bytes = tf.image.encode_jpeg(image, quality=100).numpy()\n",
    "\n",
    "    return image_bytes, height, width\n",
    "\n",
    "\n",
    "def _process_image_files_batch(thread_index, ranges, name, output_directory, dataset, num_shards, error_queue):\n",
    "    \"\"\"Processes and saves list of images as TFRecord in 1 thread.\n",
    "    Args:\n",
    "      thread_index: integer, unique batch to run index is within [0, len(ranges)).\n",
    "      ranges: list of pairs of integers specifying ranges of each batches to\n",
    "        analyze in parallel.\n",
    "      name: string, unique identifier specifying the data set (e.g. `train` or `test`)\n",
    "      output_directory: string, file path to store the tfrecord files.\n",
    "      dataset: list, a list of image example dicts\n",
    "      num_shards: integer number of shards for this data set.\n",
    "      error_queue: Queue, a queue to place image examples that failed.\n",
    "    \"\"\"\n",
    "    # Each thread produces N shards where N = int(num_shards / num_threads).\n",
    "    # For instance, if num_shards = 128, and the num_threads = 2, then the first\n",
    "    # thread would produce shards [0, 64).\n",
    "    num_threads = len(ranges)\n",
    "    assert not num_shards % num_threads\n",
    "    num_shards_per_batch = int(num_shards / num_threads)\n",
    "\n",
    "    shard_ranges = np.linspace(ranges[thread_index][0],\n",
    "                               ranges[thread_index][1],\n",
    "                               num_shards_per_batch + 1).astype(int)\n",
    "    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n",
    "\n",
    "    counter = 0\n",
    "    error_counter = 0\n",
    "    for s in range(num_shards_per_batch):\n",
    "        # Generate a sharded version of the file name, e.g. 'train-00002-of-00010'\n",
    "        shard = thread_index * num_shards_per_batch + s\n",
    "        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\n",
    "        output_file = os.path.join(output_directory, output_filename)\n",
    "        writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "        shard_counter = 0\n",
    "        files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n",
    "        for i in files_in_shard:\n",
    "\n",
    "            image_example = dataset[i]\n",
    "\n",
    "            filename = str(image_example['filename'])\n",
    "\n",
    "            try:\n",
    "                image_buffer, height, width = _process_image(filename)\n",
    "\n",
    "                example = _convert_to_example(image_example, image_buffer, height, width)\n",
    "                writer.write(example.SerializeToString())\n",
    "                shard_counter += 1\n",
    "                counter += 1\n",
    "            except Exception as e:\n",
    "                raise\n",
    "                error_counter += 1\n",
    "                error_queue.put(image_example)\n",
    "\n",
    "        shard_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_cv\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    # Parse an Example to access the Features\n",
    "    example = tf.io.parse_single_example(\n",
    "      record_bytes,\n",
    "      features={\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/colorspace': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/channels': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/label': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/bbox/count': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/bbox/score': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/parts/count': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/area': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/id': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string)\n",
    "      }\n",
    "    )\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 14:43:01.345999: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346024: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346094: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346129: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346180: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346195: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346215: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346240: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346274: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346285: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346327: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346356: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346359: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346398: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346414: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346421: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n",
      "2023-10-11 14:43:01.346463: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at example_parsing_ops"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_24_device_/job:localhost/replica:0/task:0/device:CPU:0}} Key: image/object/bbox/score.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/klb807/git/Work_Tools/bbox_comparisons/keras_cv.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/klb807/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39mchdir(\u001b[39m'\u001b[39m\u001b[39m/home/klb807/MARS_project/test/detection/black_top_tfrecords_detection\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/klb807/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m./train_dataset*\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/klb807/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTFRecordDataset(train_dataset)\u001b[39m.\u001b[39mmap(decode_fn):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/klb807/git/Work_Tools/bbox_comparisons/keras_cv.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/keras_cv_test/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:809\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    808\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 809\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    810\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    811\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras_cv_test/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:772\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 772\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    773\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    774\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    775\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    777\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras_cv_test/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3028\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3027\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 3028\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   3029\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   3030\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras_cv_test/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5888\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[1;32m   5887\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 5888\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_24_device_/job:localhost/replica:0/task:0/device:CPU:0}} Key: image/object/bbox/score.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext] name: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".cc:98 : INVALID_ARGUMENT: Key: image/object/bbox/score.  Can't parse serialized Example.\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/klb807/MARS_project/test/detection/black_top_tfrecords_detection')\n",
    "\n",
    "train_dataset = glob.glob('./train_dataset*')\n",
    "\n",
    "for batch in tf.data.TFRecordDataset(train_dataset).map(decode_fn):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=16,\n",
    "    bounding_box_format='xyxy',\n",
    "    backbone=keras_cv.models.YOLOV8Backbone.from_preset('yolo_v8_m_backbone_coco'),\n",
    "    fpn_depth=2\n",
    ")\n",
    "\n",
    "for image_fn in os.listdir('.'):\n",
    "# for image_fn in ['013009_A25_Block6_castBCma1_t_4677.png']:\n",
    "    if '.png' in image_fn:\n",
    "        with open(image_fn,'rb') as fid:\n",
    "            # bring in image data, create PIL image for later display\n",
    "            image_data = tf.image.decode_image(fid.read(),channels=3)\n",
    "            # image_data = tf.image.resize_with_crop_or_pad(image_data, 512, 512)\n",
    "            image_data = tf.image.resize_with_pad(image_data, 512, 512)\n",
    "            image_data = tf.expand_dims(image_data, axis=0)\n",
    "\n",
    "            output = mdl.predict(image_data)\n",
    "            \n",
    "            # display the image and the bounding boxes \n",
    "            im = Image.fromarray(np.squeeze(image_data.numpy()).astype(np.uint8))\n",
    "            draw = ImageDraw.Draw(im)\n",
    "            for i_box,box in enumerate(output['boxes'][0]):\n",
    "                clrs = int(16*output['classes'][0][i_box])\n",
    "                draw.rectangle(box, outline=(255-clrs, clrs, clrs, 0))\n",
    "            \n",
    "            im.show()\n",
    "            i = input()\n",
    "\n",
    "            if i.lower() == 'stop':\n",
    "                break\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython import display\n",
    "\n",
    "# im = Image.fromarray(image_data)\n",
    "# im.\n",
    "# im.show()\n",
    "\n",
    "im = Image.fromarray(np.squeeze(image_data.numpy()).astype(np.uint8))\n",
    "im.show()\n",
    "# draw = ImageDraw.Draw(im)\n",
    "# output['boxes'][0][0]\n",
    "# im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openCV_tester",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
