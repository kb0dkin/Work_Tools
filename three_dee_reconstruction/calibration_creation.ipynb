{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration Creation\n",
    "\n",
    "Run this on a calibration video, then use the created bits for behavior videos.\n",
    "\n",
    "Creates a series of opencv calibration matrices for the multi-view mouse house\n",
    "\n",
    "It also allows the user to select masks for each camera view.\n",
    "\n",
    "\n",
    "First, import all necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, glob, random\n",
    "from os import path, mkdir\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.backend_bases import MouseButton\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib ipympl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ChAruco board\n",
    "this is currently setup for a 6x6 board with a Aruco square with a vertex length of .8*chessboard tile length. It will save the board as a tiff in the desired \"workdir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '~/' # just put it in home for now\n",
    "aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_6X6_250)\n",
    "board = cv2.aruco.CharucoBoard_create(6,6,1,.8,aruco_dict)\n",
    "imboard = board.draw((2000,2000))\n",
    "cv2.imwrite(workdir+\"ChAruco.tiff\",imboard)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plt.imshow(imboard, cmap = mpl.cm.gray, interpolation='nearest')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the masks\n",
    "These masks are to split each image into the different views -- since we'll be working with mirrors, and the location of each view might change a little in each recording if the camera gets bumped or something\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/klb807/Documents/three_dee_calib_tests/'\n",
    "\n",
    "images = glob.glob(base_path+'*.tiff')\n",
    "\n",
    "num_images = 1 # how many images do we want to use for cropping?\n",
    "num_masks = 5 # how many crops (masks) do we want per image?\n",
    "\n",
    "\n",
    "class mask():\n",
    "    def __init__(self, num_masks, image_filenames):\n",
    "        self.num_images = len(image_filenames)\n",
    "        self.num_masks = num_masks\n",
    "        self.image_filenames = image_filenames\n",
    "\n",
    "        self.image_index = 0\n",
    "        self.mask_index = 0\n",
    "        self.point_index = 0\n",
    "\n",
    "        self.mask = np.ndarray((num_images, num_masks, 4, 2))\n",
    "        self.mask[:] = np.nan\n",
    "                    \n",
    "                        \n",
    "def on_click(event, mask):\n",
    "    if (event.button is MouseButton.LEFT):\n",
    "        mask.mask[mask.image_index, mask.mask_index, mask.point_index, 0] = event.xdata # x location\n",
    "        mask.mask[mask.image_index, mask.mask_index, mask.point_index, 1] = event.ydata # y location\n",
    "        ax.plot(event.xdata, event.ydata, color='forestgreen', marker='*')\n",
    "        # print(f\"x: {event.x}, x_data: {event.xdata}\")\n",
    "\n",
    "\n",
    "        # update indices for appropriate mask, and show the polygon\n",
    "        mask.point_index = np.mod(mask.point_index + 1, 4) # assume 4 points per mask\n",
    "        if mask.point_index == 0: # if we've done all of the points in a mask\n",
    "            ax.add_patch(Polygon(mask.mask[mask.image_index, mask.mask_index, :,:], color='forestgreen', alpha=.25))\n",
    "            mask.mask_index = np.mod(mask.mask_index + 1, mask.num_masks) # update mask index\n",
    "            if mask.mask_index == 0: # if we've gone through the masks for an image\n",
    "                mask.image_index += 1\n",
    "                if mask.image_index == mask.num_images: # if we've shown all of the images\n",
    "                    plt.disconnect(binding_id)\n",
    "                    print(f\"callback disconnected\")\n",
    "                    plt.close(fig)\n",
    "                        \n",
    "                else: # if we still have more images to show, display the next one\n",
    "                    # im = PIL.Image(image_filenames[image_index])\n",
    "                    ax.clear()\n",
    "                    im = cv2.imread(mask.image_filenames[mask.image_index])\n",
    "                    ax.imshow(im, cmap=mpl.cm.gray)\n",
    "\n",
    "    elif event.button is MouseButton.RIGHT:\n",
    "        plt.disconnect(binding_id)\n",
    "        print(f\"callback disconnected\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "image_filenames = random.choices(images, k=num_images)\n",
    "im = cv2.imread(image_filenames[0])\n",
    "ax.imshow(im)\n",
    "mask = mask(num_masks=num_masks, image_filenames=image_filenames)\n",
    "binding_id = plt.connect('button_press_event', lambda x: on_click(x, mask))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the average for each mask\n",
    "Find an average mask for each location, then crop all of the images in the directory according to that cropping and work from there.\n",
    "\n",
    "We can't guarantee the order of tagged points, so we'll basically find the closest point for each image and average those. \n",
    "Final array should be (num_masks)x4x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_masks = np.ndarray((num_masks, 4, 2))\n",
    "\n",
    "if num_images == 1:\n",
    "    mean_masks = mask.mask[0,:,:,:]\n",
    "else:\n",
    "    # for each mask for the first image\n",
    "    for mask_ind in range(num_masks):\n",
    "        curr_mask = mask.mask[0,mask_ind,:,:] # get the current mask\n",
    "\n",
    "        for i_point in range(4): # for each point in the current mask\n",
    "            curr_point = curr_mask[i_point, :]\n",
    "            points = np.ndarray((num_images,2)) # find the point from each image that's closest to the current point\n",
    "            points[0,:] = curr_mask[i_point, :] # setup the first point\n",
    "            for image_ind in range(1,2):\n",
    "                all_points_from_image = np.reshape(mask.mask[image_ind, :, :, :], (num_masks*4, 2))\n",
    "                point_ind = np.argmin(np.sum((all_points_from_image - curr_point)**2, axis=1))\n",
    "                points[image_ind, :] = all_points_from_image[point_ind,:]\n",
    "\n",
    "            mean_masks[mask_ind,i_point,:] = np.mean(points, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split images and save\n",
    "For this we'll \n",
    "\n",
    "1. Create a new subdirectory for each mask\n",
    "2. Split each image by each mask and save into the associated directory\n",
    "3. Display a couple of example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new directories\n",
    "subdir_names = ['mask_'+str(ii) for ii in range(num_masks)]\n",
    "for subdir in subdir_names:\n",
    "    if not path.isdir(path.join(base_path, subdir)):\n",
    "        mkdir(path.join(base_path,subdir))\n",
    "\n",
    "for i_filename, filename in enumerate(images):\n",
    "    image = cv2.imread(filename)\n",
    "    _, base_filename = path.split(filename)\n",
    "    for i_mask in range(num_masks):\n",
    "        crop = image[int(min(mean_masks[i_mask,:,1])):int(max(mean_masks[i_mask,:,1])), # x values\n",
    "                     int(min(mean_masks[i_mask,:,0])):int(max(mean_masks[i_mask,:,0])), :] # y values\n",
    "        result = cv2.imwrite(path.join(base_path,subdir_names[i_mask],base_filename), crop)\n",
    "        if result == False:\n",
    "            print(f\"Could not write image {path.join(base_path,subdir_names[i_mask],base_filename)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now get the distortion array etc for each view\n",
    "opencv fortunately has functions to do this for us\n",
    "\n",
    "we'll start by creating a function to parse the ChAruco board. This comes from tutorials from both Aruco2 and OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chessboards(images):\n",
    "    allCorners = [] # all chessboard corners\n",
    "    allIds = [] # all Aruco IDs\n",
    "    decimator = 0 # for sub-pixel estimation\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.00001) # subpixel corner criteria\n",
    "\n",
    "    for im in images:\n",
    "        frame = cv2.imread(im,0) # read it in grayscale\n",
    "        corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(frame, aruco_dict) # from definition above\n",
    "\n",
    "        if len(corners) > 0:\n",
    "            # sub pixel detection, apparently. assuming linear interpolation?\n",
    "            for corner in corners:\n",
    "                cv2.cornerSubPix(frame, corner, winSize=(3,3), zeroZone=(-1,-1), criteria=criteria) # not sure about this -- no output...\n",
    "            res2 = cv2.aruco.interpolateCornersCharuco(corners, ids, frame, board)\n",
    "            if res2[1] is not None and res2[2] is not None and len(res2[1])> 3 and decimator%1 == 0:\n",
    "                allCorners.append(res2[1])\n",
    "                allIds.append(res2[2])\n",
    "\n",
    "        \n",
    "        decimator += 1\n",
    "    \n",
    "    imsize = frame.shape\n",
    "    return allCorners, allIds, imsize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and define the calibration function -- this will return the camera matrix, distortion coefficients, rotation vectors etc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_camera(allCorners,allIds,imsize):\n",
    "    \"\"\"\n",
    "    Calibrates the camera using the detected corners.\n",
    "    \"\"\"\n",
    "\n",
    "    cameraMatrixInit = np.array([[ 1000.,    0., imsize[0]/2.],\n",
    "                                 [    0., 1000., imsize[1]/2.],\n",
    "                                 [    0.,    0.,           1.]])\n",
    "\n",
    "    distCoeffsInit = np.zeros((5,1))\n",
    "    flags = (cv2.CALIB_USE_INTRINSIC_GUESS + cv2.CALIB_RATIONAL_MODEL + cv2.CALIB_FIX_ASPECT_RATIO)\n",
    "    #flags = (cv2.CALIB_RATIONAL_MODEL)\n",
    "    (ret, camera_matrix, distortion_coefficients0,\n",
    "     rotation_vectors, translation_vectors,\n",
    "     stdDeviationsIntrinsics, stdDeviationsExtrinsics,\n",
    "     perViewErrors) = cv2.aruco.calibrateCameraCharucoExtended(\n",
    "                      charucoCorners=allCorners,\n",
    "                      charucoIds=allIds,\n",
    "                      board=board,\n",
    "                      imageSize=imsize,\n",
    "                      cameraMatrix=cameraMatrixInit,\n",
    "                      distCoeffs=distCoeffsInit,\n",
    "                      flags=flags,\n",
    "                      criteria=(cv2.TERM_CRITERIA_EPS & cv2.TERM_CRITERIA_COUNT, 10000, 1e-9))\n",
    "\n",
    "    return ret, camera_matrix, distortion_coefficients0, rotation_vectors, translation_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now run each of them for each view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(path.join(base_path,'mask_0','*.tiff'))\n",
    "\n",
    "allCorners, allIds, imsize = read_chessboards(images)\n",
    "ret, mtx, dist, rvecs, tvecs = calibrate_camera(allCorners, allIds, imsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to draw the 3d axis from the chessboard corner in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_axis(img, corners, imgpts):\n",
    "    corner = tuple(corners[0].ravel())\n",
    "    img = cv2.line(img, corner, tuple(imgpts[0].ravel()), (255,0,0), 5)\n",
    "    img = cv2.line(img, corner, tuple(imgpts[0].ravel()), (0,255,0), 5)\n",
    "    img = cv2.line(img, corner, tuple(imgpts[0].ravel()), (0,0,255), 5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.00001) # subpixel corner criteria\n",
    "objp = np.zeros((6*6, 3), np.float32)\n",
    "objp[:,:2] = np.mgrid[1:2:6, 1:2:6].T.reshape(-1,2)\n",
    "\n",
    "axis = np.float32([[3,0,0], [0,3,0], [0,0,3]]).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in glob.glob(path.join(base_path, 'mask_0', '*.tiff')):\n",
    "    img = cv2.imread(image, 0)\n",
    "    # ret, corners = cv2.findChessboardCorners(img, (6,6), None)\n",
    "    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(img, aruco_dict) # from definition above\n",
    "    # convert from list to arrays, then reshape into Nx2 (N=number of squares x 4 points per square), \n",
    "    # then sort by id\n",
    "    corner_array = np.reshape(np.array(corners),(-1,2))\n",
    "    id_reorder = \n",
    "\n",
    "    # if ret == True:\n",
    "    if len(corners) > 0:\n",
    "        corner_array = np.array(corners)\n",
    "        if [0] in ids:\n",
    "            corners2 = cv2.cornerSubPix(img, corner_array[np.where(ids==0)].squeeze(), (11,11), (-1,-1), criteria)\n",
    "\n",
    "            # find the rotation and translation vectors)\n",
    "            # ret, rvecs, tvecs = cv2.solvePnP(objp, corners2, mtx, dist)\n",
    "            ret, rvecs, tvecs = cv2.solvePnP(objp, corners2, mtx, dist)\n",
    "            # project 3d points to image plane\n",
    "            imgpts, jac = cv2.projectPoints(axis, rvecs, tvecs, mtx, dist)\n",
    "            # img = draw_axis(img, corners2, imgpts)\n",
    "            img = draw_axis(img, corners2, imgpts)\n",
    "            img_name = image[:4]+'_axis.png'\n",
    "            ret = cv2.imwrite(img_name, img)\n",
    "\n",
    "        if ret == False:\n",
    "            print(f\"could not save image {img_name}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"No board found for {image}\")\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an html template file to load on to Amazon\n",
    "specifically for the multi-view situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_AWS_multiview_template import generate_AWS_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_AWS_template(['Nose','Right Ear','Left Ear','Throat','Spine Center',\n",
    "                       'Right Front Paw','Left Front Paw', 'Right Rear Paw','Left Rear Paw',\n",
    "                       'Right Hip','Left Hip','Tail Base', 'Tail Mid', 'Tail Tip'], \n",
    "                       project_dir= 'C:\\\\Users\\\\17204\\\\Documents\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.abspath('/home/klb807/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Multi_view_AWS_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping video 1 of 1\n"
     ]
    }
   ],
   "source": [
    "Multi_view_AWS_preparation.multi_view_preparation(output_dir='C:\\\\Users\\\\17204\\\\Documents\\\\Data\\\\Cropped', num_frames=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "drawing = False # true if mouse is pressed\n",
    "mode = True # if True, draw rectangle. Press 'm' to toggle to curve\n",
    "ix,iy = -1,-1\n",
    "# mouse callback function\n",
    "def draw_circle(event,x,y,flags,param):\n",
    "    global ix,iy,drawing,mode\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix,iy = x,y\n",
    "    elif event == cv.EVENT_MOUSEMOVE:\n",
    "        if drawing == True:\n",
    "            if mode == True:\n",
    "                cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\n",
    "            else:\n",
    "                cv.circle(img,(x,y),5,(0,0,255),-1)\n",
    "    elif event == cv.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        if mode == True:\n",
    "            cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\n",
    "        else:\n",
    "            cv.circle(img,(x,y),5,(0,0,255),-1)\n",
    "\n",
    "\n",
    "img = np.zeros((512,512,3), np.uint8)\n",
    "cv.namedWindow('image')\n",
    "cv.setMouseCallback('image',draw_circle)\n",
    "while(1):\n",
    "    cv.imshow('image',img)\n",
    "    k = cv.waitKey(1) & 0xFF\n",
    "    if k == ord('m'):\n",
    "        mode = not mode\n",
    "    elif k == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.array((5,5))\n",
    "\n",
    "np.floor(np.array(test.shape)/2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "vid_read = cv2.VideoCapture('/mnt/Kennedy_SMB/ASAP/iMCI-P60induction/2596506_5678/20231024/openfield/Basler_acA1300-60gm__24108332__20231024_111226468.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_dirname, vid_filepath = path.split('/mnt/Kennedy_SMB/ASAP/iMCI-P60induction/2596506_5678/20231024/openfield/Basler_acA1300-60gm__24108332__20231024_111226468.mp4')\n",
    "vid_basename = path.splitext(vid_filepath)[0]\n",
    "\n",
    "label_frames = random.choices(range(int(vid_read.get(cv2.CAP_PROP_FRAME_COUNT))), k=int(np.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'height: 982, width: 1517'\n",
    "{'center': 319, 'east': 350, 'north': 251, 'south': 381, 'west': 336}\n",
    "{'center': 503, 'east': 548, 'north': 520, 'south': 524, 'west': 445}\n",
    "{'center': [822, 1265, 1141, 1768],\n",
    " 'east': [316, 969, 666, 1517],\n",
    " 'north': [0, 498, 251, 1018],\n",
    " 'south': [601, 496, 982, 1020],\n",
    " 'west': [323, 0, 659, 445]}\n",
    "{'center': array([ 305, 1526,  624, 2029]),\n",
    " 'east': array([ 284, 2046,  634, 2594]),\n",
    " 'north': array([  30, 1512,  281, 2032]),\n",
    " 'south': array([ 637, 1526, 1018, 2050]),\n",
    " 'west': array([ 298, 1067,  634, 1512])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/klb807/Documents/AWS_labeling_setup/boundaries.txt','r+') as fid:\n",
    "    contents = fid.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/klb807/Documents/annotation_interface.template'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgenerate_AWS_multiview_template\u001b[39;00m\n\u001b[0;32m      3\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[43mgenerate_AWS_multiview_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_AWS_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/klb807/Documents/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\17204\\Documents\\git\\Work_Tools\\three_dee_reconstruction\\generate_AWS_multiview_template.py:16\u001b[0m, in \u001b[0;36mgenerate_AWS_template\u001b[1;34m(keypoints, project_dir)\u001b[0m\n\u001b[0;32m     13\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_img\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     14\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_bounds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m360\u001b[39m,\u001b[38;5;241m640\u001b[39m,\u001b[38;5;241m720\u001b[39m],[\u001b[38;5;241m640\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1280\u001b[39m,\u001b[38;5;241m360\u001b[39m],[\u001b[38;5;241m640\u001b[39m,\u001b[38;5;241m360\u001b[39m,\u001b[38;5;241m1280\u001b[39m,\u001b[38;5;241m720\u001b[39m],[\u001b[38;5;241m640\u001b[39m,\u001b[38;5;241m720\u001b[39m,\u001b[38;5;241m1280\u001b[39m,\u001b[38;5;241m1080\u001b[39m],[\u001b[38;5;241m1280\u001b[39m,\u001b[38;5;241m360\u001b[39m,\u001b[38;5;241m1920\u001b[39m,\u001b[38;5;241m720\u001b[39m]]\n\u001b[1;32m---> 16\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mannotation_interface.template\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m<script src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://assets.crowd.aws/crowd-html-elements.js\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m></script>\u001b[39m\n\u001b[0;32m     20\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124m\u001b[39m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m     91\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(message)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/klb807/Documents/annotation_interface.template'"
     ]
    }
   ],
   "source": [
    "import generate_AWS_multiview_template\n",
    "\n",
    "keypoints = [\"Nose\",\"Throat\",\"Body Center\",\"Right Ear\",\"Left Ear\",\"RIght Hip\",\"Left Hip\",\"Tail Base\",\"Tail Mid\",\"Tail Tip\"]\n",
    "\n",
    "generate_AWS_multiview_template.generate_AWS_template(keypoints,'/home/klb807/Documents/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_read = cv2.VideoCapture('C:/Users/17204/Documents/Data/cropped/Basler_acA1300-60gm__24254439__20231026_105544912_cropped.mp4')\n",
    "ret,frame = vid_read.read()\n",
    "\n",
    "l1 = 'Label each keypoint:'\n",
    "l2 = '    - in at least 3 views'\n",
    "l3 = '    - only once per view'\n",
    "b1 = 'Refer to instructions'\n",
    "b2 = 'for view layout'\n",
    "inst_scale = 0.5\n",
    "top_size = cv2.getTextSize(l1, cv2.FONT_HERSHEY_SIMPLEX, inst_scale,1)[0]\n",
    "b1_size = cv2.getTextSize(b1, cv2.FONT_HERSHEY_SIMPLEX, inst_scale,1)[0]\n",
    "b2_size = cv2.getTextSize(b2, cv2.FONT_HERSHEY_SIMPLEX, inst_scale,1)[0]\n",
    "\n",
    "# put a header onto the frame\n",
    "header = np.zeros((5*top_size[1],frame.shape[1],frame.shape[2]))\n",
    "frame_new = np.concatenate((header,frame), axis=0)\n",
    "# and a footer\n",
    "footer = np.zeros((2*b1_size[1], frame_new.shape[1], 3))\n",
    "frame_new = np.concatenate((frame_new, footer),axis=0).astype(np.uint8)\n",
    "\n",
    "b1_origin = (int(frame.shape[1]-(b1_size[0]+5)),int(frame.shape[0] - 2*b1_size[1]))\n",
    "b2_origin = (int(frame.shape[1]-(b2_size[0]+5)),int(frame.shape[0] - 0.5*b2_size[1]))\n",
    "cv2.putText(frame, l1, (5,int(1.5*top_size[1])), cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "cv2.putText(frame, l2, (5,int(3*top_size[1])), cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "cv2.putText(frame, l3, (5,int(4.5*top_size[1])), cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "cv2.putText(frame, b1, b1_origin, cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "cv2.putText(frame, b2, b2_origin, cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "# cv2.putText(frame_new, l1, (5,int(1.5*top_size[1])), cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "# cv2.putText(frame_new, l2, (5,int(3*top_size[1])), cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "# cv2.putText(frame_new, l3, (5,int(4.5*top_size[1])), cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "# cv2.putText(frame_new, bottom_text, bottom_origin, cv2.FONT_HERSHEY_SIMPLEX, inst_scale, (255,255,255))\n",
    "                \n",
    "\n",
    "fig,ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow(frame)\n",
    "# ax[1].imshow(frame_new)\n",
    "\n",
    "cv2.imwrite('C:/Users/17204/Documents/Data/Cropped/PutText_Concat.png',frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('/home/klb807/git/Work_Tools/Reminder.drawio.png')\n",
    "\n",
    "while(1):\n",
    "    cv.imshow('image',img)\n",
    "    k = cv.waitKey(1) & 0xFF\n",
    "    if k == 27: # press esc to esc\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_thresh = ((img > 125)*255).astype(np.uint8)\n",
    "np.zeros((frame.shape[0] + top_size[1]*5,frame.shape[1],frame.shape[2])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /io/opencv/modules/highgui/src/precomp.hpp:155: error: (-215:Assertion failed) src_depth != CV_16F && src_depth != CV_32S in function 'convertToShow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mimg_thresh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     k \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m: \u001b[38;5;66;03m# press esc to esc\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /io/opencv/modules/highgui/src/precomp.hpp:155: error: (-215:Assertion failed) src_depth != CV_16F && src_depth != CV_32S in function 'convertToShow'\n"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    cv.imshow('image',img_thresh)\n",
    "    k = cv.waitKey(1) & 0xFF\n",
    "    if k == 27: # press esc to esc\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
