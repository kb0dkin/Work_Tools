{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crimson Stimulation Responses\n",
    "\n",
    "Looking through the BrainPatch stimulation responses, in particular those LFP/dendritic spikes. Need to figure out what they are...\n",
    "\n",
    "From the \"artifact_exploration\" stuff I was doing, it looks like I'll mostly need to look at clips around the stim. Can probably HPF at about 70 hz and keep the interesting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from open_ephys.analysis import Session\n",
    "from scipy import signal\n",
    "# from scipy.fft import fft, fftfreq\n",
    "# from scipy.signal.windows import gaussian\n",
    "from sklearn.decomposition import PCA\n",
    "import os, glob, re\n",
    "import openephys_utils \n",
    "\n",
    "\n",
    "# %matplotlib ipympl\n",
    "%matplotlib qt\n",
    "\n",
    "# pdfPages for saving images to a multi-page PDF\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a few functions for later usage\n",
    "\n",
    "First one just opens an open ephys directory and returns the signals, timestamps, and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_sig_events(directory:str):\n",
    "    # open up a session, then pull out the signal and events\n",
    "    session = Session(directory)\n",
    "\n",
    "    recording = session.recordnodes[0].recordings[0].continuous[0]\n",
    "\n",
    "    # get out the signal\n",
    "    sig = recording.samples[:,:64] * recording.metadata['bit_volts'][0]\n",
    "\n",
    "    # pull out the events -- both giving the time and the indices\n",
    "    events = np.argwhere(np.diff(recording.samples[:,64]>5000) == 1)\n",
    "    events = events.reshape([int(events.shape[0]/2),2])\n",
    "    event_ts = events/recording.metadata['sample_rate']\n",
    "\n",
    "    # timestamps\n",
    "    timestamps = recording.sample_numbers - recording.sample_numbers[0]\n",
    "    timestamps = timestamps/recording.metadata['sample_rate']\n",
    "\n",
    "    return sig, timestamps, events, event_ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find the minimum of a clipped period after the stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_responses(sig, events, len_ms:int = 25, n_chans:int = 64, sample_rate:int = 30000):\n",
    "\n",
    "    t_len = len_ms*30 # 52 ms * 30 khz\n",
    "\n",
    "    # set up the events to plot patches\n",
    "    n_events = events.shape[0] # number of stimulation events\n",
    "\n",
    "    responses = np.zeros((n_events, t_len, n_chans))\n",
    "    # maxs = np.zeros((n_events, n_chans)) # not getting much info from these\n",
    "    # rel_maxs = np.zeros((n_events, n_chans))\n",
    "    # abs_maxs = np.zeros((n_events, n_chans))\n",
    "    mins = np.zeros((n_events, n_chans))\n",
    "    rel_mins = np.zeros((n_events, n_chans))\n",
    "    abs_mins = np.zeros((n_events, n_chans))\n",
    "\n",
    "    for i_event, event in enumerate(events):\n",
    "        response = sig[event[0]:event[0]+len_ms*int(sample_rate/1000),:]\n",
    "        means = np.mean(sig[event[0]+4:event[1]-4,:], axis = 0)\n",
    "        responses[i_event,:,:] = response - means # response for each channel\n",
    "    \n",
    "        mins[i_event,:] = np.min(response - means, axis=0)\n",
    "        rel_mins[i_event,:] = np.argmin(response - means, axis=0)/30000\n",
    "        abs_mins[i_event,:] = rel_mins[i_event,:] + event_ts[i_event,0]\n",
    "\n",
    "    return mins, rel_mins, abs_mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average post-stim responses for a particular channel. Will plot it into an existing axis if provided one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_response(sig, events, len_ms:int = 25, channel = 0, ax:plt.axes=None, label:str=None):\n",
    "    # Plot the average response for a particular channel\n",
    "    \n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    \n",
    "    if label == None:\n",
    "        label = f'Channel {channel}'\n",
    "\n",
    "\n",
    "    # set up the events to plot patches\n",
    "    n_events = events.shape[0] # number of stimulation events\n",
    "\n",
    "    # put together a NxT array\n",
    "    t_len = len_ms * 30\n",
    "    responses = np.zeros((n_events, t_len))\n",
    "    \n",
    "    # go through each event\n",
    "    for i_event, event in enumerate(events):\n",
    "        response = sig[event[0]:event[0]+len_ms*30,channel]\n",
    "        means = np.mean(sig[event[0]+4:event[1]-4]) # center the during-stimulation to 0\n",
    "        responses[i_event,:] = response - means\n",
    "    \n",
    "    # put together the means, STDs, and timestamps\n",
    "    ts = np.arange(t_len)/30\n",
    "    means = np.mean(responses, axis=0)\n",
    "    line = ax.plot(ts, means, label=label)\n",
    "    \n",
    "    ts_std = np.ravel(np.array([ts, ts[::-1]]))\n",
    "    std = np.ravel(np.array([means + np.std(responses, axis=0), means[::-1] - np.std(responses, axis=0)[::-1]]))\n",
    "    patch_array = np.array([ts_std, std]).T\n",
    "    std_patch = Polygon(patch_array, alpha=0.2, color=line[-1].get_color())\n",
    "    ax.add_patch(std_patch)\n",
    "\n",
    "    # print(dir(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find spikes using the old-school default:\n",
    "\n",
    "1. Filter and de-mean\n",
    "1. Calculate a threshold: -4.5x the STD\n",
    "1. Flag issues:\n",
    "    1. Too-short ISIs (< 3 ms?)\n",
    "    1. Simulataneous-ish (on more than N channels)\n",
    "    1. Something about the wave shape -- deviations? Depth of field?\n",
    "1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spikes(sig, filter_high:float = 6000, filter_low:float = 150, sample_rate:int = 30000, CAR:bool = True):\n",
    "    # number of channels\n",
    "    n_chan = np.min(sig.shape)\n",
    "    axis = np.argmax(sig.shape)\n",
    "    sig = sig.T if axis == 1 else sig # make sure that Time is along axis 0\n",
    "\n",
    "    # # CAR\n",
    "    # if CAR:\n",
    "    #     pca = PCA()\n",
    "    #     xform = PCA.fit_transform(sig)\n",
    "    #     # sig = np.matmul(xform, PCA.)\n",
    "\n",
    "\n",
    "    # filter the thing\n",
    "    sos = signal.butter(N=8, Wn=[filter_low, filter_high], fs=sample_rate, output='sos', btype='bandpass')\n",
    "    filt_sig = signal.sosfiltfilt(sos=sos, x=sig, axis=0)\n",
    "\n",
    "    # find a threshold for each channel\n",
    "    thresholds = np.expand_dims(-4.5 * np.std(filt_sig, axis=0), axis=0)\n",
    "\n",
    "\n",
    "    # find the crossings\n",
    "    thresholds_rep = np.tile(thresholds, (sig.shape[0], 1)) # create a TxN array of N threshold values\n",
    "    crossings = np.argwhere(np.diff((filt_sig<thresholds_rep).astype(int), axis=0) < 0)\n",
    "\n",
    "    # Create a dataframe for the spikes, and also store chunks of 50 ms of data\n",
    "    spike_df = pd.DataFrame({'sample_no':crossings[:,0].astype(int), 'electrode':crossings[:,1].astype(int)})\n",
    "    sample_columns = [f'sample {i - 10}' for i in range(50)]\n",
    "    spike_df.loc[:,sample_columns] = np.nan\n",
    "\n",
    "    for i_row,row in spike_df.iterrows():\n",
    "        spike_df.loc[i_row,sample_columns] = filt_sig[row['sample_no']-10:row['sample_no']+40,row['electrode']]\n",
    "\n",
    "    # return spike_df, filt_sig\n",
    "    return spike_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single file analysis\n",
    "\n",
    "Mostly to check the functioning of the code when I'm batch processing files\n",
    "\n",
    "pull in the data -- we'll start with one file at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral\\\\Crimson__2024-10-02_12-21-01__20mA_2ms_400um'\n",
    "\n",
    "# load signals if we haven't already loaded it\n",
    "if 'sig' not in locals():\n",
    "    sig, timestamps, events, event_ts = open_sig_events(directory)\n",
    "\n",
    "# pull out the spikes\n",
    "if 'spike_df' not in locals():\n",
    "    spike_df,filt_sig = find_spikes(sig)\n",
    "\n",
    "# choose the channels to show\n",
    "channels = np.arange(40,50)\n",
    "\n",
    "# plot the continuous and show the times\n",
    "fig_cont, ax_cont = plt.subplots(nrows = len(channels), sharex=True)\n",
    "\n",
    "for i_channel, channel in enumerate(channels):\n",
    "    ax_cont[i_channel].plot(timestamps, filt_sig[:,channel])\n",
    "    for i_spike, spike in spike_df.loc[spike_df['electrode'] == channel].iterrows():\n",
    "        ax_cont[i_channel].axvspan((int(spike['sample_no'])-10)/30000, (int(spike['sample_no'])+40)/30000, color = 'cyan')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spike_counts(spike_dict:dict, max_ts:int = None, min_ts:int = None, fs:int = 30000, bin_ms:float = .005):\n",
    "    # calculate the binned firing rates and return, along with bin timestamps\n",
    "\n",
    "    if (max_ts is None) or (min_ts is None):\n",
    "        print('Calculating bin range start and finish is a bummer! . Next time give me some info!')\n",
    "        max_ts,min_ts = 0,0\n",
    "        for chan_xings in spike_dict.values():\n",
    "            max_ts = int(max(max_ts, chan_xings['spike_ts'].max()))\n",
    "            min_ts = int(min(min_ts, chan_xings['spike_ts'].min()))\n",
    "        \n",
    "\n",
    "    bins = np.arange(start=min_ts, step = int(fs*bin_ms), stop=max_ts+1) # put together the bins\n",
    "    spike_counts = np.empty((len(bins),len(spike_dict.keys()))) # put together a pre-allocated array\n",
    "    for channel, data in spike_dict.items(): # loop through the dict\n",
    "        spike_counts[:,channel] = np.histogram(data['sample_no'], bins) # bin it\n",
    "\n",
    "\n",
    "    return spike_counts, bins    \n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('h')\n",
    "\n",
    "max_ts= int(sig.shape[0]) + 1\n",
    "min_ts= 0\n",
    "fs = 30000\n",
    "bin_ms = .005\n",
    "\n",
    "\n",
    "bins = np.arange(start=min_ts, step = int(fs*bin_ms), stop=max_ts+1) # put together the bins\n",
    "spike_counts = np.empty((len(bins)-1,len(spikes.keys()))) # put together a pre-allocated array\n",
    "for channel, data in spikes.items(): # loop through the dict\n",
    "    spike_counts[:,channel],_ = np.histogram(data['sample_no'], bins) # bin it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading previously converted file Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-21-01__20mA_2ms_400um\\raw_signal.pkl\n"
     ]
    }
   ],
   "source": [
    "# import openephys_utils\n",
    "# fig_pca, ax_pca = plt.subplots(nrows=4, sharex=True)\n",
    "directory = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral\\\\Crimson__2024-10-02_12-21-01__20mA_2ms_400um'\n",
    "\n",
    "if not all([var in locals() for var in ['sig', 'timestamps', 'events', 'events_ts']]):\n",
    "    sig, timestamps, events, event_ts = openephys_utils.open_sig_events(directory)\n",
    "\n",
    "sig_eraasr = openephys_utils.ERAASR(sig)\n",
    "sig_mine = openephys_utils.ERAASR(sig, mode='mine')\n",
    "\n",
    "spikes = openephys_utils.threshold_crossings(sig_eraasr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openephys_utils\n",
    "spike_dict = openephys_utils.threshold_crossings(sig_eraasr, multi_rejection=None)\n",
    "fr2,fr2_ts = openephys_utils.calc_FR(spike_dict, max_samp = sig.shape[0], min_samp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fr, fr_ts = openephys_utils.calc_FR(spikes, max_samp = sig.shape[0], min_samp=0)\n",
    "\n",
    "# channels = [32, 36, 39, 48, 51] # the mapping from Sara just seems to be 1:1, but I'm not sure that's right...\n",
    "# fig_clean,ax_clean = plt.subplots(nrows = len(channels), sharex=True, sharey=True)\n",
    "# for i_channel,channel in enumerate(channels):\n",
    "#     ax_clean[i_channel].plot(timestamps,sig[:,channel], label=f'{channel} raw')\n",
    "#     ax_clean[i_channel].plot(timestamps,sig_eraasr[:,channel], label=f'{channel} eraasr')\n",
    "#     ax_clean[i_channel].plot(timestamps,sig_mine[:,channel], label=f'{channel} mine')\n",
    "\n",
    "    # put the spike times\n",
    "    # marker = np.std(sig_eraasr[:,channel])*5 * np.ones((spikes[i_channel]['sample_no'].shape[0],1))\n",
    "    # ax_clean[i_channel].scatter(marker, timestamps[spikes[i_channel]['sample_no']])\n",
    "\n",
    "    # ax_clean[i_channel].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spike_binary(spike_dict:dict, ax = None, stims:np.array = None, fs = 30000):\n",
    "    '''\n",
    "    Plot an on/off of channels over time. show the stimulations if given\n",
    "    '''\n",
    "\n",
    "    # create an axis if not given\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "\n",
    "    # for each channel, plot the spike times\n",
    "    for channel, data in spike_dict.items():\n",
    "        ax.vlines(data['sample_no']/fs, channel, channel+1)\n",
    "    \n",
    "    # plot the stimulation times if given\n",
    "    if stims is not None:\n",
    "        for i_stim in range(stims.shape[0]):\n",
    "            patch_array = np.array([[stims[i_stim,0]/fs,-1],\n",
    "                                        [stims[i_stim,1]/fs,-1],\n",
    "                                        [stims[i_stim,1]/fs,len(spike_dict.keys())],\n",
    "                                        [stims[i_stim,0]/fs,len(spike_dict.keys())]])\n",
    "            stim_patch = Polygon(patch_array, alpha=0.2, color='k')\n",
    "            ax.add_patch(stim_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PSTH(spike_dict:dict, stims:np.array, channel:int = 0, ax=None, fs=30000):\n",
    "    '''\n",
    "    Plot the PSTH for a single channel\n",
    "    \n",
    "    inputs:\n",
    "        spike_dict:dict     - firing rates\n",
    "        events:np.array     - event times\n",
    "        channel:int         - which channel are we working with?\n",
    "    '''\n",
    "\n",
    "    # create an axis if not given\n",
    "    if ax == None:\n",
    "        fig,ax = plt.subplots()\n",
    "\n",
    "    # split the spikes for the channel into a series of new channels\n",
    "    spikes = spike_dict[channel]['sample_no']\n",
    "    for i_stim in range(stims.shape[0]-1):\n",
    "        spike_subset = spikes[np.logical_and(spikes>=stims[i_stim,0], spikes<stims[i_stim+1,0])] - stims[i_stim,0]\n",
    "        ax.vlines(spike_subset/fs,i_stim,i_stim+1)\n",
    "        # create a patch at the stimulus point\n",
    "        patch_array = np.array([[0, i_stim], [stims[i_stim,1]/fs-stims[i_stim,0]/fs,i_stim],\n",
    "                                [stims[i_stim,1]/fs-stims[i_stim,0]/fs,i_stim+1], [0, i_stim+1]])\n",
    "        stim_patch = Polygon(patch_array, alpha=0.4, color='k')\n",
    "        ax.add_patch(stim_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spike_subset = spikes[51]['sample_no']\n",
    "for i_stim in range(events.shape[0]-1):\n",
    "        resh = spike_subset[np.logical_and(spike_subset>=events[i_stim,0], spike_subset<events[i_stim+1,0])] - events[i_stim,0]\n",
    "        ax.vlines(spike_subset,i_stim,i_stim+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_waveforms(spike_dict:dict, channel:int=0, fs:int=30000, ax=None):\n",
    "    '''\n",
    "    plot the mean threshold crossing for a channel, and a patch around the standard deviation.\n",
    "\n",
    "    could theoretically look at splitting into different units\n",
    "\n",
    "    inputs:\n",
    "        spike_dict\n",
    "        channels\n",
    "        std_flag\n",
    "        map\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    # create an axis if it doesn't exist\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "\n",
    "    # go into the waveforms\n",
    "    mean_wf = spike_dict[channel]['waveform'].mean(axis=0)\n",
    "    std_= spike_dict[channel]['waveform'].std(axis=0)\n",
    "\n",
    "    # elapsed ts\n",
    "    ts = [t/(fs/1000) for t in range(mean_wf.shape[0])]\n",
    "\n",
    "    # create std patch\n",
    "    std_array = np.ndarray((2*mean_wf.shape[0],2))\n",
    "    std_array[:mean_wf.shape[0],:] = np.column_stack((ts, mean_wf+std_))\n",
    "    std_array[mean_wf.shape[0]:,:] = np.column_stack((ts, mean_wf-std_))[::-1,:]\n",
    "    std_patch = Polygon(std_array, alpha=0.2, color = 'orange')\n",
    "\n",
    "    # plot em\n",
    "    ax.plot(ts, mean_wf, color='orange') \n",
    "    ax.add_patch(std_patch)\n",
    "    ax.set_xlabel('time (ms)')\n",
    "    ax.set_ylabel('magnitude (uV)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spike_binary(spikes, stims = events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multisave_PDF(fig, pdf):\n",
    "    pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "type(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(directory, 'channel_plots_minimum_nomultichannel.pdf')) as pdf:\n",
    "    fig, ax = plt.subplots(nrows = 2)\n",
    "    for channel in range(64):\n",
    "        # plot_PSTH(spikes, events, ax=ax[0], channel=channel)\n",
    "        # plot_mean_waveforms(spikes, ax=ax[1], channel=channel)\n",
    "        pdf.attach_note(f'Channel {channel}')\n",
    "        plot_PSTH(spike_dict, events, ax=ax[0], channel=channel)\n",
    "        plot_mean_waveforms(spike_dict, ax=ax[1], channel=channel)\n",
    "        pdf.savefig(fig)\n",
    "        for sub_ax in ax:\n",
    "            sub_ax.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_pass, low_pass = 300, 6000\n",
    "fs = 30000\n",
    "thresh_mult = -3.5\n",
    "\n",
    "sos_bpf = signal.butter(N = 8, Wn = [high_pass, low_pass], btype='bandpass', fs = fs, output='sos')\n",
    "sig_filt = signal.sosfiltfilt(sos=sos_bpf, x = sig_eraasr, axis=0)\n",
    "\n",
    "# find the threshold values for each channel\n",
    "thresholds = np.std(sig_filt, axis=0) * thresh_mult\n",
    "xings = np.nonzero(np.diff(np.where(sig_filt < thresholds, 1, 0), axis=0) == 1)\n",
    "\n",
    "# fig,ax = plt.subplots(nrows = len(channels))\n",
    "\n",
    "# for i_channel,channel in enumerate(channels):\n",
    "#     xings_channel = xings[0][xings[1] == channel]\n",
    "#     ax[i_channel].plot(timestamps, sig_filt[:,channel])\n",
    "#     ax[i_channel].plot(timestamps, sig_eraasr[:,channel])\n",
    "#     ax[i_channel].hlines(thresholds[channel], timestamps[0], timestamps[-1])\n",
    "\n",
    "#     ax[i_channel].vlines(timestamps[xings_channel], 1.2*np.min(sig_filt[:,channel]), 1.2*np.max(sig_filt[:,channel]))\n",
    "    \n",
    "\n",
    "# need to introduce some basic cross-channel artifact rejection\n",
    "\n",
    "# split into per-channel dictionary\n",
    "# bt = int(.0003*fs)\n",
    "# at = int(.0012 * fs)\n",
    "# spike_dict = {}\n",
    "# for i_channel in np.arange(sig.shape[1]):\n",
    "#     spike_ts = xings[0][xings[1] == i_channel] # sample #\n",
    "#     spike_wf = [sig_filt[ts-bt:ts+at,i_channel] for ts in spike_ts] # waveform\n",
    "\n",
    "#     spike_dict[i_channel] = {'sample_no':spike_ts, 'waveform':spike_wf}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [32, 36, 39, 48, 51] # the mapping from Sara just seems to be 1:1, but I'm not sure that's right...\n",
    "fig_filt, ax_filt = plt.subplots(nrows = len(channels), sharex=True, sharey=True)\n",
    "\n",
    "for i_channel, channel in enumerate(channels):\n",
    "    ax_filt[i_channel].plot(timestamps,sig_clean[:,channel])\n",
    "    ax_filt[i_channel].plot(timestamps,sig_filt[:,channel])\n",
    "    ax_filt[i_channel].hlines(np.std(sig_filt[:,channel])*-3.5, timestamps[0], timestamps[-1])\n",
    "    ax_filt[i_channel].set_title(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_13-29-59__20mA_MinOil_2ms'\n",
    "# directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_13-46-01__20mA_MinOil_2ms'\n",
    "# directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_15-10-03__20mA_MinOil_2ms'\n",
    "\n",
    "\n",
    "# directory = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral\\\\Crimson__2024-10-02_12-21-01__20mA_2ms_400um'\n",
    "directory = 'Z:\\\\BrainPatch\\\\20241002\\\\Crimson__2024-10-02_12-00-49__spontaneous_waking'\n",
    "\n",
    "# # load signals if we haven't already loaded it\n",
    "# if 'sig' not in locals():\n",
    "#     sig, timestamps, events, event_ts = open_sig_events(directory)\n",
    "session = Session(directory)\n",
    "print(session)\n",
    "\n",
    "\n",
    "for i_rec in range(len(session.recordnodes)):\n",
    "    print(f'{len(session.recordnodes[i_rec].recordings)} recording(s) in session \"{session.recordnodes[i_rec].directory}\"\\n')\n",
    "    recordings = session.recordnodes[i_rec].recordings\n",
    "    \n",
    "    for i_rec,recording in enumerate(recordings):\n",
    "        recording.load_continuous()\n",
    "        recording.load_spikes()\n",
    "        recording.load_events()\n",
    "        recording.load_messages()\n",
    "\n",
    "        print(f'Recording {i_rec} has:')\n",
    "        print(f'\\t{len(recording.continuous)} continuous streams')\n",
    "        print(f'\\t{len(recording.spikes)} spike streams')\n",
    "        print(f'\\t{len(recording.events)} event streams')\n",
    "    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the offline filtering to the online filtering, and take a look at the specific channels that I think might have some good stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel list -- \n",
    "channels = [32, 36, 39, 48, 51] # the mapping from Sara just seems to be 1:1, but I'm not sure that's right...\n",
    "\n",
    "# put together some filters\n",
    "# sos_h = signal.butter(N = 8, Wn = [150], btype = 'high', output = 'sos', fs=30000)\n",
    "# sos_l = signal.butter(N = 8, Wn = [6000], btype = 'low', output = 'sos', fs=30000)\n",
    "sos_bp = signal.butter(N=4, Wn = [150, 8000], btype='bandpass', output='sos', fs=30000)\n",
    "\n",
    "# timestamps -- raw\n",
    "# ts_raw = np.arange(len(session.recordnodes[0].recordings[0].continuous[0].sample_numbers))/session.recordnodes[0].recordings[0].continuous[0].metadata['sample_rate']\n",
    "ts_raw = session.recordnodes[0].recordings[0].continuous[0].sample_numbers/session.recordnodes[0].recordings[0].continuous[0].metadata['sample_rate']\n",
    "# ts_filt = np.arange(len(session.recordnodes[0].recordings[1].continuous[0].sample_numbers))/session.recordnodes[0].recordings[1].continuous[0].metadata['sample_rate']\n",
    "# ts_filt = session.recordnodes[0].recordings[1].continuous[0].sample_numbers/session.recordnodes[0].recordings[1].continuous[0].metadata['sample_rate']\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(nrows=len(channels), sharex=True)\n",
    "# raw recording -- filter it and plot it\n",
    "for i_channel, channel in enumerate(channels):\n",
    "    # sig_temp = signal.sosfilt(sos_l, signal.sosfilt(sos_h, session.recordnodes[0].recordings[0].continuous[0].samples[:,channel])/4)\n",
    "    sig_temp = signal.sosfilt(sos_bp, session.recordnodes[0].recordings[0].continuous[0].samples[:,channel])\n",
    "    ax[i_channel].plot(ts_raw, sig_temp, label='filtered offline')\n",
    "    ax[i_channel].plot(ts_raw, session.recordnodes[0].recordings[0].continuous[0].samples[:,channel], label='raw')\n",
    "    # ax[i_channel].plot(ts_filt, session.recordnodes[0].recordings[1].continuous[0].samples[:,channel], label='filtered online')\n",
    "\n",
    "    ax[i_channel].set_ylabel('uV')\n",
    "    ax[i_channel].legend()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_freq, ax_freq = plt.subplots(nrows=2)\n",
    "\n",
    "w, h = signal.sosfreqz(sos=sos_bp, fs = 30000)\n",
    "\n",
    "ax_freq[0].semilogx(w, 20*np.log10(np.abs(h)))\n",
    "ax_freq[1].semilogx(w, np.angle(h))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_map = loadmat(\"Z:\\\\BrainPatch\\\\20241002\\\\64-4shank-poly-brainpatch-chanMap.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab 50 ms after each stimulation. Set the mean of the stimulation period to 0.\n",
    "\n",
    "Find the minimum, maximum, depth of modulation, and time of each after the stimulation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chans = 64 # 64 recording channels\n",
    "len_ms = 150\n",
    "t_len = len_ms*30 # 52 ms * 30 khz\n",
    "n_events = events.shape[0] # number of stimulation events\n",
    "\n",
    "# set up the events to plot patches\n",
    "events = np.argwhere(np.diff(recording.continuous[0].samples[:,64]>5000) == 1)\n",
    "events = events.reshape([int(events.shape[0]/2),2])\n",
    "event_ts = events/recording.continuous[0].metadata['sample_rate']\n",
    "\n",
    "responses = np.zeros((n_events, t_len, n_chans))\n",
    "maxs = np.zeros((n_events, n_chans))\n",
    "rel_maxs = np.zeros((n_events, n_chans))\n",
    "abs_maxs = np.zeros((n_events, n_chans))\n",
    "mins = np.zeros((n_events, n_chans))\n",
    "rel_mins = np.zeros((n_events, n_chans))\n",
    "abs_mins = np.zeros((n_events, n_chans))\n",
    "\n",
    "for i_event, event in enumerate(events):\n",
    "    response = sig[event[0]:event[0]+len_ms*30,:]\n",
    "    means = np.mean(sig[event[0]+4:event[1]-4,:], axis = 0)\n",
    "    responses[i_event,:,:] = response - means # response for each channel\n",
    "    \n",
    "    mins[i_event,:] = np.min(response - means, axis=0)\n",
    "    rel_mins[i_event,:] = np.argmin(response - means, axis=0)/30000\n",
    "    abs_mins[i_event,:] = rel_mins[i_event,:] + event_ts[i_event,0]\n",
    "\n",
    "    # maxs[i_event,:] = np.max(response[int(rel_mins*30000),:] - means, axis=0) # only interested in stuff after the negative deviation\n",
    "    # rel_maxs[i_event,:] = np.argmax(response[int(rel_mins*30000),:] - means, axis=0)/30000\n",
    "    # abs_maxs[i_event,:] = rel_maxs[i_event,:] + event_ts[i_event,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing, but look at the same channel for a couple of different stimulation amplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average stimulation responses\n",
    "\n",
    "let's take a look at the average stimulation response for a couple different electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_13-46-01__20mA_MinOil_2ms'\n",
    "directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_13-29-59__20mA_MinOil_2ms'\n",
    "\n",
    "# get the signal etc\n",
    "signal, timestamps, events, event_ts = open_sig_events(directory)\n",
    "\n",
    "fig_avg, ax_avg = plt.subplots()\n",
    "\n",
    "for channel in [0,5,10,15,20]:\n",
    "    plot_avg_response(signal, events, len_ms= 40, channel=channel, ax=ax_avg)\n",
    "\n",
    "\n",
    "ax_avg.axvspan(0, 2, color='k', alpha=.1, label='Stimulation Period')\n",
    "\n",
    "# clean up the plot, add a legend etc\n",
    "ax_avg.legend()\n",
    "for spine in ['top','bottom','right','left']:\n",
    "    ax_avg.spines[spine].set_visible(False)\n",
    "\n",
    "ax_avg.set_xlabel('Time after stimulation onset (ms)')\n",
    "ax_avg.set_ylabel('Magnitude (uV)')\n",
    "ax_avg.set_title('Mean stimulation responses with standard deviations\\n20 mA, 400 um')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-file analysis\n",
    "\n",
    "Looking at the responses over different distances and currents\n",
    "\n",
    "First we need to put together a list of the different recordings and the parameters\n",
    "\n",
    "### August 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## August 21 data\n",
    "# lets go through recordings in groups of locations\n",
    "base_dir = 'Z:\\\\BrainPatch\\\\20240821'\n",
    "\n",
    "dir_400 = ['Crimson__2024-08-21_13-44-07__10mA_MinOil_2ms','Crimson__2024-08-21_13-46-01__20mA_MinOil_2ms','Crimson__2024-08-21_13-47-40__15mA_MinOil_2ms','Crimson__2024-08-21_13-49-43__10mA_MinOil_2ms','Crimson__2024-08-21_13-51-50__5mA_MinOil_2ms']\n",
    "dir_700 = ['Crimson__2024-08-21_13-56-49__5mA_MinOil_2ms','Crimson__2024-08-21_13-58-50__10mA_MinOil_2ms','Crimson__2024-08-21_14-00-53__15mA_MinOil_2ms','Crimson__2024-08-21_14-02-54__20mA_MinOil_2ms']\n",
    "dir_1000 = ['Crimson__2024-08-21_14-05-52__5mA_MinOil_2ms','Crimson__2024-08-21_14-07-41__10mA_MinOil_2ms','Crimson__2024-08-21_14-09-46__15mA_MinOil_2ms','Crimson__2024-08-21_14-11-45__20mA_MinOil_2ms']\n",
    "dir_1300 = ['Crimson__2024-08-21_14-14-26__5mA_MinOil_2ms','Crimson__2024-08-21_14-16-02__10mA_MinOil_2ms','Crimson__2024-08-21_14-17-58__15mA_MinOil_2ms','Crimson__2024-08-21_14-20-21__20mA_MinOil_2ms']\n",
    "dir_1600 = ['Crimson__2024-08-21_14-23-13__5mA_MinOil_2ms','Crimson__2024-08-21_14-25-16__10mA_MinOil_2ms','Crimson__2024-08-21_14-27-12__15mA_MinOil_2ms','Crimson__2024-08-21_14-29-03__20mA_MinOil_2ms']\n",
    "\n",
    "# dictionary of direct groups\n",
    "dir_dict = {400: dir_400, 700:dir_700, 1000:dir_1000, 1300:dir_1300, 1600:dir_1600}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### September 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = 'Z:\\\\BrainPatch\\\\20240925\\\\No_Mineral_Oil'\n",
    "base_dir = 'Z:\\\\BrainPatch\\\\20240925'\n",
    "\n",
    "dir_400 = glob.glob('*0mm_2ms*', root_dir=base_dir) + glob.glob('*2ms_0mm*', root_dir=base_dir)\n",
    "dir_600 = glob.glob('*2ms_.6mm', root_dir=base_dir) \n",
    "dir_1200 = glob.glob('*2ms_1.2mm', root_dir=base_dir) \n",
    "dir_1500 = glob.glob('*2ms_1.5mm', root_dir=base_dir) \n",
    "\n",
    "dir_dict = {400:dir_400, 600:dir_600, 1200:dir_1200, 1500:dir_1500}\n",
    "\n",
    "channel = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### October 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral'\n",
    "\n",
    "dir_300 = glob.glob('*2ms_400um', root_dir=base_dir)\n",
    "dir_600 = glob.glob('*2ms_600um', root_dir=base_dir)\n",
    "dir_900 = glob.glob('*2ms_900um', root_dir=base_dir)\n",
    "dir_1200 = glob.glob('*2ms_1200us', root_dir=base_dir)\n",
    "dir_1500 = glob.glob('*2ms_1500um', root_dir=base_dir)\n",
    "\n",
    "dir_dict = {300:dir_300, 600:dir_600, 900:dir_900, 1200:dir_1200, 1500:dir_1500}\n",
    "\n",
    "channel = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's take a look at a single channel for a few different current levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_avg_dist, ax_avg_dist = plt.subplots()\n",
    "\n",
    "channel = 48\n",
    "distance = 1500\n",
    "\n",
    "for sub_dir in dir_1500:\n",
    "    directory = os.path.join(base_dir,sub_dir)\n",
    "\n",
    "    # get the signal etc\n",
    "    sig, timestamps, events, event_ts = open_sig_events(directory)\n",
    "\n",
    "    amp = re.search('(\\d)+mA',sub_dir)[0]\n",
    "    plot_avg_response(sig, events, len_ms= 40, channel=channel, ax=ax_avg_dist, label=amp)\n",
    "\n",
    "ax_avg_dist.axvspan(0, 2, color='k', alpha=.1, label='Stimulation Period')\n",
    "    \n",
    "# clean up the plot, add a legend etc\n",
    "ax_avg_dist.legend()\n",
    "for spine in ['top','bottom','right','left']:\n",
    "    ax_avg_dist.spines[spine].set_visible(False)\n",
    "\n",
    "ax_avg_dist.set_xlabel('Time after stimulation onset (ms)')\n",
    "ax_avg_dist.set_ylabel('Magnitude (uV)')\n",
    "ax_avg_dist.set_title(f'Mean stimulation at different stimulation amplitudes\\nChannel {channel}, {distance} um')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all of the different directories, then put the mean and median negative deviation for each channel into a dataframe for easy analysis and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_df = pd.DataFrame(columns=['Channel_no','Current','Distance','uMin','uMin_ts','medMin','medMin_ts'])\n",
    "\n",
    "for dist,dir_list in dir_dict.items():\n",
    "    for sub_dir in dir_list:\n",
    "        directory = os.path.join(base_dir, sub_dir) # go through the subdir, check to make sure it exists and that there's data inside\n",
    "        if not os.path.exists(directory):\n",
    "            continue\n",
    "        if not len([file for file in os.listdir(directory) if not file.startswith('.')]): # if the directory is empty, skip it\n",
    "            continue\n",
    "\n",
    "\n",
    "        # open the directory\n",
    "        sig, timestamps, events, event_ts = open_sig_events(directory)\n",
    "\n",
    "        # pull out the stim responses\n",
    "        mins, rel_mins, abs_mins = find_responses(sig, events)\n",
    "\n",
    "        # means and medians for each channel\n",
    "        uMins = np.mean(mins, axis=0)\n",
    "        uMins_ts = np.mean(rel_mins, axis=0)\n",
    "        medMins = np.median(mins, axis=0)\n",
    "        medMins_ts = np.median(rel_mins, axis=0)\n",
    "\n",
    "        # a nested dictionary of all of the channels responses\n",
    "        tdict = {ii:{'Channel_no':ii, \n",
    "                'Current':re.search('([0-9]+)mA', sub_dir)[1],\n",
    "                'Distance': dist,\n",
    "                'uMin':uMins[ii],\n",
    "                'uMin_ts':uMins_ts[ii],\n",
    "                'medMin':medMins[ii],\n",
    "                'medMin_ts':medMins_ts[ii],\n",
    "                } for ii in range(64)}\n",
    "\n",
    "        t_df = pd.DataFrame.from_dict(tdict, orient='index') # create a dataframe\n",
    "\n",
    "        resp_df = pd.concat([resp_df, t_df], ignore_index=True)\n",
    "\n",
    "resp_df.Current = resp_df.Current.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the effects of distance on the magnitude of the response for the different current levels. Different channels on different axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currents = resp_df.Current.unique()\n",
    "currents.sort()\n",
    "# channels = [10, 15, 20, 25, 30]\n",
    "channels = np.arange(64, step = 5)\n",
    "\n",
    "fig_dist,ax_dist = plt.subplots(nrows=len(channels), sharex=True, sharey=True)\n",
    "# fig_time, ax_time = plt.subplots(nrows=len(channels), sharex=True, sharey=True)\n",
    "for i_chan,chan in enumerate(channels):\n",
    "    for i_curr,curr in enumerate(currents):\n",
    "        dist_cmp = resp_df.loc[(resp_df.Current==curr) * (resp_df.Channel_no==chan)]\n",
    "        ax_dist[i_chan].plot(dist_cmp.Distance, dist_cmp.uMin)\n",
    "        # ax_time[i_chan].plot(dist_cmp.Distance, dist_cmp.uMin_ts)\n",
    "\n",
    "    ax_dist[i_chan].legend([f'{current} mA' for current in currents], loc=4)\n",
    "    ax_dist[i_chan].set_title(f'Channel {chan}')\n",
    "    ax_dist[i_chan].set_ylabel('Magnitude (uV)')\n",
    "\n",
    "\n",
    "    # ax_time[i_chan].legend([f'{current} mA' for current in currents], loc=4)\n",
    "    # ax_time[i_chan].set_title(f'Channel {chan}')\n",
    "    # ax_time[i_chan].set_ylabel('Time (ms)')\n",
    "\n",
    "    # remove the outer boxes\n",
    "    for spine in ['top','bottom','right','left']:\n",
    "        ax_dist[i_chan].spines[spine].set_visible(False)\n",
    "        # ax_time[i_chan].spines[spine].set_visible(False)\n",
    "    \n",
    "\n",
    "ax_dist[-1].set_xlabel('Distance (um)')\n",
    "fig_dist.suptitle('Mean response minimum as a function of distance (per current level)')\n",
    "\n",
    "\n",
    "# ax_time[-1].set_xlabel('Distance (um)')\n",
    "# fig_time.suptitle('Mean minimum time as a function of distance (per current level)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean negative deviation for all channels as a function of distance. Different axis per current level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currents = resp_df.Current.unique()\n",
    "distances = resp_df.Distance.unique()\n",
    "currents.sort()\n",
    "distances.sort()\n",
    "\n",
    "fig_min_scatter,ax_min_scatter = plt.subplots(ncols=len(currents), sharex=True, sharey=True)\n",
    "for i_curr,curr in enumerate(currents):\n",
    "    dist_cmp = resp_df.loc[resp_df.Current==curr ]\n",
    "    ax_min_scatter[i_curr].scatter(dist_cmp.Distance, dist_cmp.uMin, s = 2, color='grey')\n",
    "    current_means = dist_cmp.groupby('Distance').mean('uMin')\n",
    "    ax_min_scatter[i_curr].plot(current_means.index, current_means.uMin, color='k')\n",
    "\n",
    "    # ax_min_scatter[i_curr].legend([f'{current} mA' for current in currents], loc=4)\n",
    "    ax_min_scatter[i_curr].set_title(f'LED current: {curr} mA')\n",
    "    ax_min_scatter[i_curr].set_xlabel('Distance (um)')\n",
    "\n",
    "\n",
    "    # remove the outer boxes\n",
    "    for spine in ['top','bottom','right','left']:\n",
    "        ax_min_scatter[i_curr].spines[spine].set_visible(False)\n",
    "        # ax_time[i_chan].spines[spine].set_visible(False)\n",
    "    \n",
    "\n",
    "ax_min_scatter[0].set_ylabel('Magnitude (uV)')\n",
    "fig_dist.suptitle('Mean response minimum as a function of distance (per current level)')\n",
    "\n",
    "\n",
    "# ax_time[-1].set_xlabel('Distance (um)')\n",
    "# fig_time.suptitle('Mean minimum time as a function of distance (per current level)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time of the minimum value as a function of current. Each distance on a different plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currents = resp_df.Current.unique()\n",
    "distances = resp_df.Distance.unique()\n",
    "currents.sort()\n",
    "distances.sort()\n",
    "\n",
    "fig_time_scatter,ax_time_scatter = plt.subplots(ncols=len(distances), sharex=True, sharey=True)\n",
    "for i_dist,dist in enumerate(distances):\n",
    "    curr_cmp = resp_df.loc[resp_df.Distance == dist]\n",
    "    ax_time_scatter[i_dist].scatter(curr_cmp.Current, curr_cmp.uMin_ts, s = 2, color='blue')\n",
    "    curr_means = curr_cmp.groupby('Current').mean('uMin_ts')\n",
    "    ax_time_scatter[i_dist].plot(curr_means.index, curr_means.uMin_ts, color='k')\n",
    "\n",
    "    # ax_time_scatter[i_dist].legend([f'{distent} mA' for distent in distents], loc=4)\n",
    "    ax_time_scatter[i_dist].set_title(f'Distance: {dist}um')\n",
    "    ax_time_scatter[i_dist].set_xlabel('Current (mA)')\n",
    "\n",
    "\n",
    "    # remove the outer boxes\n",
    "    for spine in ['top','bottom','right','left']:\n",
    "        ax_time_scatter[i_dist].spines[spine].set_visible(False)\n",
    "        # ax_time[i_chan].spines[spine].set_visible(False)\n",
    "    \n",
    "\n",
    "ax_time_scatter[0].set_ylabel('Time (ms)')\n",
    "fig_dist.suptitle('Mean response minimum as a function of current (per distance)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Channel spike processing\n",
    "\n",
    "\n",
    "October 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral'\n",
    "\n",
    "dir_300 = glob.glob(os.path.join(base_dir,'*2ms_400um'))\n",
    "dir_600 = glob.glob(os.path.join(base_dir,'*2ms_600um'))\n",
    "dir_900 = glob.glob(os.path.join(base_dir,'*2ms_900um'))\n",
    "dir_1200 = glob.glob(os.path.join(base_dir,'*2ms_1200us'))\n",
    "dir_1500 = glob.glob(os.path.join(base_dir,'*2ms_1500um'))\n",
    "\n",
    "dir_dict = {300:dir_300, 600:dir_600, 900:dir_900, 1200:dir_1200, 1500:dir_1500}\n",
    "\n",
    "channel = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading previously converted file Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-21-01__20mA_2ms_400um\\raw_signal.pkl\n",
      "saving data to Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-24-31__20mA_2ms_400um\\raw_signal.pkl\n",
      "saving data to Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-26-46__20mA_2ms_400um\\raw_signal.pkl\n",
      "saving data to Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-35-37__15mA_2ms_400um\\raw_signal.pkl\n",
      "saving data to Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-37-44__15mA_2ms_400um\\raw_signal.pkl\n",
      "saving data to Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-39-41__10mA_2ms_400um\\raw_signal.pkl\n",
      "saving data to Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-41-41__10mA_2ms_400um\\raw_signal.pkl\n",
      "saving data to Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-49-56__5mA_2ms_400um\\raw_signal.pkl\n",
      "saving data to Z:\\BrainPatch\\20241002\\lateral\\Crimson__2024-10-02_12-52-58__15mA_2ms_400um\\raw_signal.pkl\n"
     ]
    }
   ],
   "source": [
    "channels = [51]\n",
    "with PdfPages(os.path.join(base_dir, f'channel_{channel}.pdf')) as pdf:\n",
    "    fig, ax = plt.subplots(nrows = 2)\n",
    "\n",
    "    for directory in dir_300:\n",
    "        amplitude = re.search('\\d{1,2}mA', directory)[0]\n",
    "        if not all([var in locals() for var in ['sig', 'timestamps', 'events', 'events_ts']]):\n",
    "            sig, timestamps, events, event_ts = openephys_utils.open_sig_events(directory)\n",
    "\n",
    "        sig_eraasr = openephys_utils.ERAASR(sig)\n",
    "        sig_mine = openephys_utils.ERAASR(sig, mode='mine')\n",
    "\n",
    "        spike_dict = openephys_utils.threshold_crossings(sig_eraasr, multi_rejection=None, low_cutoff=-20)\n",
    "\n",
    "        for channel in channels:\n",
    "            openephys_utils.plot_mean_waveforms(spike_dict, ax=ax[0], channel=channel)\n",
    "            ax[0].set_title('Threshold Crossing Waveform')\n",
    "            openephys_utils.plot_PSTH(spike_dict, events, ax=ax[1], channel=channel)\n",
    "            ax[1].set_title('')\n",
    "            ax[1].set_xlabel('Time (s)')\n",
    "            ax[1].set_ylabel('Stimulation Number')\n",
    "            # plt.title(f'Channel {channel}, {directory}')\n",
    "            fig.text(0.05, 0.95, f'Channel {channel}, {amplitude}', transform=fig.transFigure, size=24)\n",
    "            pdf.savefig(fig)\n",
    "            for sub_ax in ax:\n",
    "                sub_ax.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15mA'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('(\\d{1,2}mA)', directory)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kilosort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
