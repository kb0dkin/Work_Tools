{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crimson Stimulation Responses\n",
    "\n",
    "Looking through the BrainPatch stimulation responses, in particular those LFP/dendritic spikes. Need to figure out what they are...\n",
    "\n",
    "From the \"artifact_exploration\" stuff I was doing, it looks like I'll mostly need to look at clips around the stim. Can probably HPF at about 70 hz and keep the interesting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from open_ephys.analysis import Session\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal.windows import gaussian\n",
    "from sklearn.decomposition import PCA\n",
    "import os, glob, re\n",
    "from openephys_utils import open_sig_events\n",
    "\n",
    "# %matplotlib ipympl\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a few functions for later usage\n",
    "\n",
    "First one just opens an open ephys directory and returns the signals, timestamps, and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_sig_events(directory:str):\n",
    "    # open up a session, then pull out the signal and events\n",
    "    session = Session(directory)\n",
    "\n",
    "    recording = session.recordnodes[0].recordings[0].continuous[0]\n",
    "\n",
    "    # get out the signal\n",
    "    sig = recording.samples[:,:64] * recording.metadata['bit_volts'][0]\n",
    "\n",
    "    # pull out the events -- both giving the time and the indices\n",
    "    events = np.argwhere(np.diff(recording.samples[:,64]>5000) == 1)\n",
    "    events = events.reshape([int(events.shape[0]/2),2])\n",
    "    event_ts = events/recording.metadata['sample_rate']\n",
    "\n",
    "    # timestamps\n",
    "    timestamps = recording.sample_numbers - recording.sample_numbers[0]\n",
    "    timestamps = timestamps/recording.metadata['sample_rate']\n",
    "\n",
    "    return sig, timestamps, events, event_ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find the minimum of a clipped period after the stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_responses(sig, events, len_ms:int = 25, n_chans:int = 64, sample_rate:int = 30000):\n",
    "\n",
    "    t_len = len_ms*30 # 52 ms * 30 khz\n",
    "\n",
    "    # set up the events to plot patches\n",
    "    n_events = events.shape[0] # number of stimulation events\n",
    "\n",
    "    responses = np.zeros((n_events, t_len, n_chans))\n",
    "    # maxs = np.zeros((n_events, n_chans)) # not getting much info from these\n",
    "    # rel_maxs = np.zeros((n_events, n_chans))\n",
    "    # abs_maxs = np.zeros((n_events, n_chans))\n",
    "    mins = np.zeros((n_events, n_chans))\n",
    "    rel_mins = np.zeros((n_events, n_chans))\n",
    "    abs_mins = np.zeros((n_events, n_chans))\n",
    "\n",
    "    for i_event, event in enumerate(events):\n",
    "        response = sig[event[0]:event[0]+len_ms*int(sample_rate/1000),:]\n",
    "        means = np.mean(sig[event[0]+4:event[1]-4,:], axis = 0)\n",
    "        responses[i_event,:,:] = response - means # response for each channel\n",
    "    \n",
    "        mins[i_event,:] = np.min(response - means, axis=0)\n",
    "        rel_mins[i_event,:] = np.argmin(response - means, axis=0)/30000\n",
    "        abs_mins[i_event,:] = rel_mins[i_event,:] + event_ts[i_event,0]\n",
    "\n",
    "    return mins, rel_mins, abs_mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average post-stim responses for a particular channel. Will plot it into an existing axis if provided one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_response(sig, events, len_ms:int = 25, channel = 0, ax:plt.axes=None, label:str=None):\n",
    "    # Plot the average response for a particular channel\n",
    "    \n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    \n",
    "    if label == None:\n",
    "        label = f'Channel {channel}'\n",
    "\n",
    "\n",
    "    # set up the events to plot patches\n",
    "    n_events = events.shape[0] # number of stimulation events\n",
    "\n",
    "    # put together a NxT array\n",
    "    t_len = len_ms * 30\n",
    "    responses = np.zeros((n_events, t_len))\n",
    "    \n",
    "    # go through each event\n",
    "    for i_event, event in enumerate(events):\n",
    "        response = sig[event[0]:event[0]+len_ms*30,channel]\n",
    "        means = np.mean(sig[event[0]+4:event[1]-4]) # center the during-stimulation to 0\n",
    "        responses[i_event,:] = response - means\n",
    "    \n",
    "    # put together the means, STDs, and timestamps\n",
    "    ts = np.arange(t_len)/30\n",
    "    means = np.mean(responses, axis=0)\n",
    "    line = ax.plot(ts, means, label=label)\n",
    "    \n",
    "    ts_std = np.ravel(np.array([ts, ts[::-1]]))\n",
    "    std = np.ravel(np.array([means + np.std(responses, axis=0), means[::-1] - np.std(responses, axis=0)[::-1]]))\n",
    "    patch_array = np.array([ts_std, std]).T\n",
    "    std_patch = Polygon(patch_array, alpha=0.2, color=line[-1].get_color())\n",
    "    ax.add_patch(std_patch)\n",
    "\n",
    "    # print(dir(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find spikes using the old-school default:\n",
    "\n",
    "1. Filter and de-mean\n",
    "1. Calculate a threshold: -4.5x the STD\n",
    "1. Flag issues:\n",
    "    1. Too-short ISIs (< 3 ms?)\n",
    "    1. Simulataneous-ish (on more than N channels)\n",
    "    1. Something about the wave shape -- deviations? Depth of field?\n",
    "1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spikes(sig, filter_high:float = 6000, filter_low:float = 150, sample_rate:int = 30000, CAR:bool = True):\n",
    "    # number of channels\n",
    "    n_chan = np.min(sig.shape)\n",
    "    axis = np.argmax(sig.shape)\n",
    "    sig = sig.T if axis == 1 else sig # make sure that Time is along axis 0\n",
    "\n",
    "    # # CAR\n",
    "    # if CAR:\n",
    "    #     pca = PCA()\n",
    "    #     xform = PCA.fit_transform(sig)\n",
    "    #     # sig = np.matmul(xform, PCA.)\n",
    "\n",
    "\n",
    "    # filter the thing\n",
    "    sos = signal.butter(N=8, Wn=[filter_low, filter_high], fs=sample_rate, output='sos', btype='bandpass')\n",
    "    filt_sig = signal.sosfiltfilt(sos=sos, x=sig, axis=0)\n",
    "\n",
    "    # find a threshold for each channel\n",
    "    thresholds = np.expand_dims(-4.5 * np.std(filt_sig, axis=0), axis=0)\n",
    "\n",
    "\n",
    "    # find the crossings\n",
    "    thresholds_rep = np.tile(thresholds, (sig.shape[0], 1)) # create a TxN array of N threshold values\n",
    "    crossings = np.argwhere(np.diff((filt_sig<thresholds_rep).astype(int), axis=0) < 0)\n",
    "\n",
    "    # Create a dataframe for the spikes, and also store chunks of 50 ms of data\n",
    "    spike_df = pd.DataFrame({'sample_no':crossings[:,0].astype(int), 'electrode':crossings[:,1].astype(int)})\n",
    "    sample_columns = [f'sample {i - 10}' for i in range(50)]\n",
    "    spike_df.loc[:,sample_columns] = np.nan\n",
    "\n",
    "    for i_row,row in spike_df.iterrows():\n",
    "        spike_df.loc[i_row,sample_columns] = filt_sig[row['sample_no']-10:row['sample_no']+40,row['electrode']]\n",
    "\n",
    "    # return spike_df, filt_sig\n",
    "    return spike_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single file analysis\n",
    "\n",
    "Mostly to check the functioning of the code when I'm batch processing files\n",
    "\n",
    "pull in the data -- we'll start with one file at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral\\\\Crimson__2024-10-02_12-21-01__20mA_2ms_400um'\n",
    "\n",
    "# load signals if we haven't already loaded it\n",
    "if 'sig' not in locals():\n",
    "    sig, timestamps, events, event_ts = open_sig_events(directory)\n",
    "\n",
    "# pull out the spikes\n",
    "if 'spike_df' not in locals():\n",
    "    spike_df,filt_sig = find_spikes(sig)\n",
    "\n",
    "# choose the channels to show\n",
    "channels = np.arange(40,50)\n",
    "\n",
    "# plot the continuous and show the times\n",
    "fig_cont, ax_cont = plt.subplots(nrows = len(channels), sharex=True)\n",
    "\n",
    "for i_channel, channel in enumerate(channels):\n",
    "    ax_cont[i_channel].plot(timestamps, filt_sig[:,channel])\n",
    "    for i_spike, spike in spike_df.loc[spike_df['electrode'] == channel].iterrows():\n",
    "        ax_cont[i_channel].axvspan((int(spike['sample_no'])-10)/30000, (int(spike['sample_no'])+40)/30000, color = 'cyan')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_pca, ax_pca = plt.subplots(nrows=4, sharex=True)\n",
    "\n",
    "recording = session.recordnodes[0].recordings[0].continuous[0].samples\n",
    "\n",
    "sos = signal.butter(N=2, Wn = [10], fs = 30000, btype = 'high', output='sos')\n",
    "filt_sig = signal.sosfiltfilt(sos, recording[:,:64])\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(filt_sig)\n",
    "\n",
    "# ax_pca[0].plot(np.matmul(filt_sig[:,0], pca.components_[:,0]))\n",
    "# ax_pca[1].plot(np.matmul(filt_sig, pca.components_[:,1]))\n",
    "\n",
    "\n",
    "# ax_pca[0].plot(sig[:,36])\n",
    "# ax_pca[0].plot(np.matmul(sig_comp[:,2:], pca.components_[2:,:])[:,36])\n",
    "# ax_pca[1].plot(sig[:,41])\n",
    "# ax_pca[1].plot(np.matmul(sig_comp[:,2:], pca.components_[2:,:])[:,41])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_13-29-59__20mA_MinOil_2ms'\n",
    "# directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_13-46-01__20mA_MinOil_2ms'\n",
    "# directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_15-10-03__20mA_MinOil_2ms'\n",
    "\n",
    "\n",
    "# directory = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral\\\\Crimson__2024-10-02_12-21-01__20mA_2ms_400um'\n",
    "directory = 'Z:\\\\BrainPatch\\\\20241002\\\\Crimson__2024-10-02_12-00-49__spontaneous_waking'\n",
    "\n",
    "# # load signals if we haven't already loaded it\n",
    "# if 'sig' not in locals():\n",
    "#     sig, timestamps, events, event_ts = open_sig_events(directory)\n",
    "session = Session(directory)\n",
    "print(session)\n",
    "\n",
    "\n",
    "for i_rec in range(len(session.recordnodes)):\n",
    "    print(f'{len(session.recordnodes[i_rec].recordings)} recording(s) in session \"{session.recordnodes[i_rec].directory}\"\\n')\n",
    "    recordings = session.recordnodes[i_rec].recordings\n",
    "    \n",
    "    for i_rec,recording in enumerate(recordings):\n",
    "        recording.load_continuous()\n",
    "        recording.load_spikes()\n",
    "        recording.load_events()\n",
    "        recording.load_messages()\n",
    "\n",
    "        print(f'Recording {i_rec} has:')\n",
    "        print(f'\\t{len(recording.continuous)} continuous streams')\n",
    "        print(f'\\t{len(recording.spikes)} spike streams')\n",
    "        print(f'\\t{len(recording.events)} event streams')\n",
    "    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the offline filtering to the online filtering, and take a look at the specific channels that I think might have some good stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel list -- \n",
    "channels = [32, 36, 39, 48, 51] # the mapping from Sara just seems to be 1:1, but I'm not sure that's right...\n",
    "\n",
    "# put together some filters\n",
    "# sos_h = signal.butter(N = 8, Wn = [150], btype = 'high', output = 'sos', fs=30000)\n",
    "# sos_l = signal.butter(N = 8, Wn = [6000], btype = 'low', output = 'sos', fs=30000)\n",
    "sos_bp = signal.butter(N=4, Wn = [150, 8000], btype='bandpass', output='sos', fs=30000)\n",
    "\n",
    "# timestamps -- raw\n",
    "# ts_raw = np.arange(len(session.recordnodes[0].recordings[0].continuous[0].sample_numbers))/session.recordnodes[0].recordings[0].continuous[0].metadata['sample_rate']\n",
    "ts_raw = session.recordnodes[0].recordings[0].continuous[0].sample_numbers/session.recordnodes[0].recordings[0].continuous[0].metadata['sample_rate']\n",
    "# ts_filt = np.arange(len(session.recordnodes[0].recordings[1].continuous[0].sample_numbers))/session.recordnodes[0].recordings[1].continuous[0].metadata['sample_rate']\n",
    "# ts_filt = session.recordnodes[0].recordings[1].continuous[0].sample_numbers/session.recordnodes[0].recordings[1].continuous[0].metadata['sample_rate']\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(nrows=len(channels), sharex=True)\n",
    "# raw recording -- filter it and plot it\n",
    "for i_channel, channel in enumerate(channels):\n",
    "    # sig_temp = signal.sosfilt(sos_l, signal.sosfilt(sos_h, session.recordnodes[0].recordings[0].continuous[0].samples[:,channel])/4)\n",
    "    sig_temp = signal.sosfilt(sos_bp, session.recordnodes[0].recordings[0].continuous[0].samples[:,channel])\n",
    "    ax[i_channel].plot(ts_raw, sig_temp, label='filtered offline')\n",
    "    ax[i_channel].plot(ts_raw, session.recordnodes[0].recordings[0].continuous[0].samples[:,channel], label='raw')\n",
    "    # ax[i_channel].plot(ts_filt, session.recordnodes[0].recordings[1].continuous[0].samples[:,channel], label='filtered online')\n",
    "\n",
    "    ax[i_channel].set_ylabel('uV')\n",
    "    ax[i_channel].legend()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_freq, ax_freq = plt.subplots(nrows=2)\n",
    "\n",
    "w, h = signal.sosfreqz(sos=sos_bp, fs = 30000)\n",
    "\n",
    "ax_freq[0].semilogx(w, 20*np.log10(np.abs(h)))\n",
    "ax_freq[1].semilogx(w, np.angle(h))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_map = loadmat(\"Z:\\\\BrainPatch\\\\20241002\\\\64-4shank-poly-brainpatch-chanMap.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if we can filter out some of the respiratory noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = session.recordnodes[0].recordings[0].continuous[0].samples[:,:64]\n",
    "stim = session.recordnodes[0].recordings[0].continuous[0].samples[:,64]\n",
    "\n",
    "sos_h = signal.butter(N = 8, Wn = [300], btype='high', output = 'sos', fs=30000)\n",
    "sos_l = signal.butter(N = 8, Wn = [6000], btype='low', output = 'sos', fs=30000)\n",
    "sig_hpf = signal.sosfiltfilt(sos_h, sig, axis=0)\n",
    "sig_lpf = signal.sosfiltfilt(sos_l, sig, axis=0)\n",
    "# fs,mag = signal.sosfreqz(sos_l, fs=30000, worN=1024)\n",
    "\n",
    "# fig_filt, ax_filt = plt.subplots()\n",
    "\n",
    "# ax_filt.semilogx(fs,np.abs(mag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "timestamps = recording.continuous[0].sample_numbers - recording.continuous[0].sample_numbers[0]\n",
    "timestamps = timestamps/recording.continuous[0].metadata['sample_rate']\n",
    "\n",
    "# ax.loglog(fs,np.abs(mag))\n",
    "# ax.plot(timestamps, sig_hpf[:,0])\n",
    "# ax.plot(timestamps, sig_lpf[:,0])\n",
    "# ax.plot(timestamps, sig[:,0])\n",
    "ax.plot(sig_hpf[:,0])\n",
    "ax.plot(sig_lpf[:,0])\n",
    "ax.plot(sig[:,0])\n",
    "\n",
    "ax.legend(['hpf','lpf','no filt'])\n",
    "\n",
    "# set up the events to plot patches\n",
    "events = np.argwhere(np.diff(recording.continuous[0].samples[:,64]>5000) == 1)\n",
    "events = events.reshape([int(events.shape[0]/2),2])\n",
    "event_ts = events/recording.continuous[0].metadata['sample_rate']\n",
    "\n",
    "# plot the event times\n",
    "for i_event,event in enumerate(event_ts):\n",
    "    # print(f'{i_event}:{event}')\n",
    "    ax.axvspan(event[0], event[1], color='k', alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.diff(recording.continuous[0].sample_numbers) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab 50 ms after each stimulation. Set the mean of the stimulation period to 0.\n",
    "\n",
    "Find the minimum, maximum, depth of modulation, and time of each after the stimulation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chans = 64 # 64 recording channels\n",
    "len_ms = 150\n",
    "t_len = len_ms*30 # 52 ms * 30 khz\n",
    "n_events = events.shape[0] # number of stimulation events\n",
    "\n",
    "# set up the events to plot patches\n",
    "events = np.argwhere(np.diff(recording.continuous[0].samples[:,64]>5000) == 1)\n",
    "events = events.reshape([int(events.shape[0]/2),2])\n",
    "event_ts = events/recording.continuous[0].metadata['sample_rate']\n",
    "\n",
    "responses = np.zeros((n_events, t_len, n_chans))\n",
    "maxs = np.zeros((n_events, n_chans))\n",
    "rel_maxs = np.zeros((n_events, n_chans))\n",
    "abs_maxs = np.zeros((n_events, n_chans))\n",
    "mins = np.zeros((n_events, n_chans))\n",
    "rel_mins = np.zeros((n_events, n_chans))\n",
    "abs_mins = np.zeros((n_events, n_chans))\n",
    "\n",
    "for i_event, event in enumerate(events):\n",
    "    response = sig[event[0]:event[0]+len_ms*30,:]\n",
    "    means = np.mean(sig[event[0]+4:event[1]-4,:], axis = 0)\n",
    "    responses[i_event,:,:] = response - means # response for each channel\n",
    "    \n",
    "    mins[i_event,:] = np.min(response - means, axis=0)\n",
    "    rel_mins[i_event,:] = np.argmin(response - means, axis=0)/30000\n",
    "    abs_mins[i_event,:] = rel_mins[i_event,:] + event_ts[i_event,0]\n",
    "\n",
    "    # maxs[i_event,:] = np.max(response[int(rel_mins*30000),:] - means, axis=0) # only interested in stuff after the negative deviation\n",
    "    # rel_maxs[i_event,:] = np.argmax(response[int(rel_mins*30000),:] - means, axis=0)/30000\n",
    "    # abs_maxs[i_event,:] = rel_maxs[i_event,:] + event_ts[i_event,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing, but look at the same channel for a couple of different stimulation amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_time, ax_time = plt.subplots()\n",
    "\n",
    "ax_time.plot(np.diff(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_chans = [30, 31, 32, 33]\n",
    "n_plt = len(plt_chans)\n",
    "\n",
    "fig_comb,ax_comb = plt.subplots(nrows=n_plt, sharex=True, sharey = True)\n",
    "\n",
    "# put together the timestamps\n",
    "timestamps = recording.continuous[0].sample_numbers - recording.continuous[0].sample_numbers[0]\n",
    "timestamps = timestamps/recording.continuous[0].metadata['sample_rate']\n",
    "\n",
    "for i_chan, chan in enumerate(plt_chans):\n",
    "    ax_comb[i_chan].plot(timestamps, sig_filt[:,i_chan])\n",
    "\n",
    "    # plot the event times and max and min points\n",
    "    for i_event,event in enumerate(event_ts):\n",
    "        # print(f'{i_event}:{event}')\n",
    "        ax_comb[i_chan].axvspan(event[0], event[1], color='k', alpha=.1)\n",
    "        ax_comb[i_chan].scatter(abs_maxs[i_event, i_chan], maxs[i_event, i_chan], 2, color='r')\n",
    "        ax_comb[i_chan].scatter(abs_mins[i_event, i_chan], mins[i_event, i_chan], 2, color='cyan')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average stimulation responses\n",
    "\n",
    "let's take a look at the average stimulation response for a couple different electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_13-46-01__20mA_MinOil_2ms'\n",
    "directory = 'Z:\\\\BrainPatch\\\\20240821\\\\Crimson__2024-08-21_13-29-59__20mA_MinOil_2ms'\n",
    "\n",
    "# get the signal etc\n",
    "signal, timestamps, events, event_ts = open_sig_events(directory)\n",
    "\n",
    "fig_avg, ax_avg = plt.subplots()\n",
    "\n",
    "for channel in [0,5,10,15,20]:\n",
    "    plot_avg_response(signal, events, len_ms= 40, channel=channel, ax=ax_avg)\n",
    "\n",
    "\n",
    "ax_avg.axvspan(0, 2, color='k', alpha=.1, label='Stimulation Period')\n",
    "\n",
    "# clean up the plot, add a legend etc\n",
    "ax_avg.legend()\n",
    "for spine in ['top','bottom','right','left']:\n",
    "    ax_avg.spines[spine].set_visible(False)\n",
    "\n",
    "ax_avg.set_xlabel('Time after stimulation onset (ms)')\n",
    "ax_avg.set_ylabel('Magnitude (uV)')\n",
    "ax_avg.set_title('Mean stimulation responses with standard deviations\\n20 mA, 400 um')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-file analysis\n",
    "\n",
    "Looking at the responses over different distances and currents\n",
    "\n",
    "First we need to put together a list of the different recordings and the parameters\n",
    "\n",
    "### August 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## August 21 data\n",
    "# lets go through recordings in groups of locations\n",
    "base_dir = 'Z:\\\\BrainPatch\\\\20240821'\n",
    "\n",
    "dir_400 = ['Crimson__2024-08-21_13-44-07__10mA_MinOil_2ms','Crimson__2024-08-21_13-46-01__20mA_MinOil_2ms','Crimson__2024-08-21_13-47-40__15mA_MinOil_2ms','Crimson__2024-08-21_13-49-43__10mA_MinOil_2ms','Crimson__2024-08-21_13-51-50__5mA_MinOil_2ms']\n",
    "dir_700 = ['Crimson__2024-08-21_13-56-49__5mA_MinOil_2ms','Crimson__2024-08-21_13-58-50__10mA_MinOil_2ms','Crimson__2024-08-21_14-00-53__15mA_MinOil_2ms','Crimson__2024-08-21_14-02-54__20mA_MinOil_2ms']\n",
    "dir_1000 = ['Crimson__2024-08-21_14-05-52__5mA_MinOil_2ms','Crimson__2024-08-21_14-07-41__10mA_MinOil_2ms','Crimson__2024-08-21_14-09-46__15mA_MinOil_2ms','Crimson__2024-08-21_14-11-45__20mA_MinOil_2ms']\n",
    "dir_1300 = ['Crimson__2024-08-21_14-14-26__5mA_MinOil_2ms','Crimson__2024-08-21_14-16-02__10mA_MinOil_2ms','Crimson__2024-08-21_14-17-58__15mA_MinOil_2ms','Crimson__2024-08-21_14-20-21__20mA_MinOil_2ms']\n",
    "dir_1600 = ['Crimson__2024-08-21_14-23-13__5mA_MinOil_2ms','Crimson__2024-08-21_14-25-16__10mA_MinOil_2ms','Crimson__2024-08-21_14-27-12__15mA_MinOil_2ms','Crimson__2024-08-21_14-29-03__20mA_MinOil_2ms']\n",
    "\n",
    "# dictionary of direct groups\n",
    "dir_dict = {400: dir_400, 700:dir_700, 1000:dir_1000, 1300:dir_1300, 1600:dir_1600}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### September 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = 'Z:\\\\BrainPatch\\\\20240925\\\\No_Mineral_Oil'\n",
    "base_dir = 'Z:\\\\BrainPatch\\\\20240925'\n",
    "\n",
    "dir_400 = glob.glob('*0mm_2ms*', root_dir=base_dir) + glob.glob('*2ms_0mm*', root_dir=base_dir)\n",
    "dir_600 = glob.glob('*2ms_.6mm', root_dir=base_dir) \n",
    "dir_1200 = glob.glob('*2ms_1.2mm', root_dir=base_dir) \n",
    "dir_1500 = glob.glob('*2ms_1.5mm', root_dir=base_dir) \n",
    "\n",
    "dir_dict = {400:dir_400, 600:dir_600, 1200:dir_1200, 1500:dir_1500}\n",
    "\n",
    "channel = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### October 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral'\n",
    "\n",
    "dir_300 = glob.glob('*2ms_400um', root_dir=base_dir)\n",
    "dir_600 = glob.glob('*2ms_600um', root_dir=base_dir)\n",
    "dir_900 = glob.glob('*2ms_900um', root_dir=base_dir)\n",
    "dir_1200 = glob.glob('*2ms_1200us', root_dir=base_dir)\n",
    "dir_1500 = glob.glob('*2ms_1500um', root_dir=base_dir)\n",
    "\n",
    "dir_dict = {300:dir_300, 600:dir_600, 900:dir_900, 1200:dir_1200, 1500:dir_1500}\n",
    "\n",
    "channel = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's take a look at a single channel for a few different current levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_avg_dist, ax_avg_dist = plt.subplots()\n",
    "\n",
    "channel = 48\n",
    "distance = 1500\n",
    "\n",
    "for sub_dir in dir_1500:\n",
    "    directory = os.path.join(base_dir,sub_dir)\n",
    "\n",
    "    # get the signal etc\n",
    "    sig, timestamps, events, event_ts = open_sig_events(directory)\n",
    "\n",
    "    amp = re.search('(\\d)+mA',sub_dir)[0]\n",
    "    plot_avg_response(sig, events, len_ms= 40, channel=channel, ax=ax_avg_dist, label=amp)\n",
    "\n",
    "ax_avg_dist.axvspan(0, 2, color='k', alpha=.1, label='Stimulation Period')\n",
    "    \n",
    "# clean up the plot, add a legend etc\n",
    "ax_avg_dist.legend()\n",
    "for spine in ['top','bottom','right','left']:\n",
    "    ax_avg_dist.spines[spine].set_visible(False)\n",
    "\n",
    "ax_avg_dist.set_xlabel('Time after stimulation onset (ms)')\n",
    "ax_avg_dist.set_ylabel('Magnitude (uV)')\n",
    "ax_avg_dist.set_title(f'Mean stimulation at different stimulation amplitudes\\nChannel {channel}, {distance} um')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all of the different directories, then put the mean and median negative deviation for each channel into a dataframe for easy analysis and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_df = pd.DataFrame(columns=['Channel_no','Current','Distance','uMin','uMin_ts','medMin','medMin_ts'])\n",
    "\n",
    "for dist,dir_list in dir_dict.items():\n",
    "    for sub_dir in dir_list:\n",
    "        directory = os.path.join(base_dir, sub_dir) # go through the subdir, check to make sure it exists and that there's data inside\n",
    "        if not os.path.exists(directory):\n",
    "            continue\n",
    "        if not len([file for file in os.listdir(directory) if not file.startswith('.')]): # if the directory is empty, skip it\n",
    "            continue\n",
    "\n",
    "\n",
    "        # open the directory\n",
    "        sig, timestamps, events, event_ts = open_sig_events(directory)\n",
    "\n",
    "        # pull out the stim responses\n",
    "        mins, rel_mins, abs_mins = find_responses(sig, events)\n",
    "\n",
    "        # means and medians for each channel\n",
    "        uMins = np.mean(mins, axis=0)\n",
    "        uMins_ts = np.mean(rel_mins, axis=0)\n",
    "        medMins = np.median(mins, axis=0)\n",
    "        medMins_ts = np.median(rel_mins, axis=0)\n",
    "\n",
    "        # a nested dictionary of all of the channels responses\n",
    "        tdict = {ii:{'Channel_no':ii, \n",
    "                'Current':re.search('([0-9]+)mA', sub_dir)[1],\n",
    "                'Distance': dist,\n",
    "                'uMin':uMins[ii],\n",
    "                'uMin_ts':uMins_ts[ii],\n",
    "                'medMin':medMins[ii],\n",
    "                'medMin_ts':medMins_ts[ii],\n",
    "                } for ii in range(64)}\n",
    "\n",
    "        t_df = pd.DataFrame.from_dict(tdict, orient='index') # create a dataframe\n",
    "\n",
    "        resp_df = pd.concat([resp_df, t_df], ignore_index=True)\n",
    "\n",
    "resp_df.Current = resp_df.Current.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the effects of distance on the magnitude of the response for the different current levels. Different channels on different axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currents = resp_df.Current.unique()\n",
    "currents.sort()\n",
    "# channels = [10, 15, 20, 25, 30]\n",
    "channels = np.arange(64, step = 5)\n",
    "\n",
    "fig_dist,ax_dist = plt.subplots(nrows=len(channels), sharex=True, sharey=True)\n",
    "# fig_time, ax_time = plt.subplots(nrows=len(channels), sharex=True, sharey=True)\n",
    "for i_chan,chan in enumerate(channels):\n",
    "    for i_curr,curr in enumerate(currents):\n",
    "        dist_cmp = resp_df.loc[(resp_df.Current==curr) * (resp_df.Channel_no==chan)]\n",
    "        ax_dist[i_chan].plot(dist_cmp.Distance, dist_cmp.uMin)\n",
    "        # ax_time[i_chan].plot(dist_cmp.Distance, dist_cmp.uMin_ts)\n",
    "\n",
    "    ax_dist[i_chan].legend([f'{current} mA' for current in currents], loc=4)\n",
    "    ax_dist[i_chan].set_title(f'Channel {chan}')\n",
    "    ax_dist[i_chan].set_ylabel('Magnitude (uV)')\n",
    "\n",
    "\n",
    "    # ax_time[i_chan].legend([f'{current} mA' for current in currents], loc=4)\n",
    "    # ax_time[i_chan].set_title(f'Channel {chan}')\n",
    "    # ax_time[i_chan].set_ylabel('Time (ms)')\n",
    "\n",
    "    # remove the outer boxes\n",
    "    for spine in ['top','bottom','right','left']:\n",
    "        ax_dist[i_chan].spines[spine].set_visible(False)\n",
    "        # ax_time[i_chan].spines[spine].set_visible(False)\n",
    "    \n",
    "\n",
    "ax_dist[-1].set_xlabel('Distance (um)')\n",
    "fig_dist.suptitle('Mean response minimum as a function of distance (per current level)')\n",
    "\n",
    "\n",
    "# ax_time[-1].set_xlabel('Distance (um)')\n",
    "# fig_time.suptitle('Mean minimum time as a function of distance (per current level)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean negative deviation for all channels as a function of distance. Different axis per current level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currents = resp_df.Current.unique()\n",
    "distances = resp_df.Distance.unique()\n",
    "currents.sort()\n",
    "distances.sort()\n",
    "\n",
    "fig_min_scatter,ax_min_scatter = plt.subplots(ncols=len(currents), sharex=True, sharey=True)\n",
    "for i_curr,curr in enumerate(currents):\n",
    "    dist_cmp = resp_df.loc[resp_df.Current==curr ]\n",
    "    ax_min_scatter[i_curr].scatter(dist_cmp.Distance, dist_cmp.uMin, s = 2, color='grey')\n",
    "    current_means = dist_cmp.groupby('Distance').mean('uMin')\n",
    "    ax_min_scatter[i_curr].plot(current_means.index, current_means.uMin, color='k')\n",
    "\n",
    "    # ax_min_scatter[i_curr].legend([f'{current} mA' for current in currents], loc=4)\n",
    "    ax_min_scatter[i_curr].set_title(f'LED current: {curr} mA')\n",
    "    ax_min_scatter[i_curr].set_xlabel('Distance (um)')\n",
    "\n",
    "\n",
    "    # remove the outer boxes\n",
    "    for spine in ['top','bottom','right','left']:\n",
    "        ax_min_scatter[i_curr].spines[spine].set_visible(False)\n",
    "        # ax_time[i_chan].spines[spine].set_visible(False)\n",
    "    \n",
    "\n",
    "ax_min_scatter[0].set_ylabel('Magnitude (uV)')\n",
    "fig_dist.suptitle('Mean response minimum as a function of distance (per current level)')\n",
    "\n",
    "\n",
    "# ax_time[-1].set_xlabel('Distance (um)')\n",
    "# fig_time.suptitle('Mean minimum time as a function of distance (per current level)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time of the minimum value as a function of current. Each distance on a different plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currents = resp_df.Current.unique()\n",
    "distances = resp_df.Distance.unique()\n",
    "currents.sort()\n",
    "distances.sort()\n",
    "\n",
    "fig_time_scatter,ax_time_scatter = plt.subplots(ncols=len(distances), sharex=True, sharey=True)\n",
    "for i_dist,dist in enumerate(distances):\n",
    "    curr_cmp = resp_df.loc[resp_df.Distance == dist]\n",
    "    ax_time_scatter[i_dist].scatter(curr_cmp.Current, curr_cmp.uMin_ts, s = 2, color='blue')\n",
    "    curr_means = curr_cmp.groupby('Current').mean('uMin_ts')\n",
    "    ax_time_scatter[i_dist].plot(curr_means.index, curr_means.uMin_ts, color='k')\n",
    "\n",
    "    # ax_time_scatter[i_dist].legend([f'{distent} mA' for distent in distents], loc=4)\n",
    "    ax_time_scatter[i_dist].set_title(f'Distance: {dist}um')\n",
    "    ax_time_scatter[i_dist].set_xlabel('Current (mA)')\n",
    "\n",
    "\n",
    "    # remove the outer boxes\n",
    "    for spine in ['top','bottom','right','left']:\n",
    "        ax_time_scatter[i_dist].spines[spine].set_visible(False)\n",
    "        # ax_time[i_chan].spines[spine].set_visible(False)\n",
    "    \n",
    "\n",
    "ax_time_scatter[0].set_ylabel('Time (ms)')\n",
    "fig_dist.suptitle('Mean response minimum as a function of current (per distance)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(recording.continuous[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(recording.continuous[0].global_timestamps)\n",
    "recording.continuous[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openephys_utils\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "directory = 'Z:\\\\BrainPatch\\\\20241002\\\\lateral\\\\Crimson__2024-10-02_12-21-01__20mA_2ms_400um'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig, tt, stm, stm_ts = openephys_utils.open_sig_events(directory, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_high = 150\n",
    "b_low = 6000\n",
    "thresh = -3\n",
    "fs = 30000\n",
    "\n",
    "sig = sig - np.mean(sig, axis=1)[:,np.newaxis]\n",
    "filt_sos = signal.butter(N = 4, Wn = [b_high, b_low], btype = 'bandpass', fs = fs, output='sos')\n",
    "sig_filt = signal.sosfiltfilt(sos = filt_sos, x = sig, axis = 0)\n",
    "\n",
    "\n",
    "# thresholding\n",
    "thresholds = np.std(sig_filt, axis=0) * thresh # find the threshold value for each channel\n",
    "xings = np.where(np.diff((sig_filt < thresholds).astype(int)) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(sig_filt, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "ax[0].plot(tt, sig_filt[:,32])\n",
    "ax[0].plot(tt, sig[:,32])\n",
    "ax[0].hlines(thresholds[32],tt[0],tt[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ax[1].plot(tt, sig_filt[:,32])\n",
    "ax[1].plot(tt, sig[:,32])\n",
    "ax[1].hlines(thresholds[32],tt[0],tt[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca.fit(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(tt, sig[:,32]-np.median(sig, axis=1))\n",
    "ax.plot(tt, sig[:,32] - np.mean(sig, axis=1))\n",
    "ax.plot(tt, pca.transform(sig)[:,:2])\n",
    "\n",
    "pca."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainpatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
