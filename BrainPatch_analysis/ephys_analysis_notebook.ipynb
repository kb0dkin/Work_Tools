{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainPatch in-vivo electrophysiology code\n",
    "All of the following code is for analysis of the in-vivo recordings \n",
    "\n",
    "## Import python packages\n",
    "All of these should be working properly if you've used the provided conda environment file. However, if there are any versioning issues just reach out to me on github @kb0dkin and we'll get it sorted out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\__init__.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    }
   ],
   "source": [
    "import ephys_utils # loading and basic processing code.\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Polygon # this is a nice way to show error bars and standard deviations\n",
    "\n",
    "# data analysis standards\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import os, glob, re\n",
    "from pathlib import Path\n",
    "\n",
    "# csv writing\n",
    "import csv\n",
    "\n",
    "# tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# open the plots with QT\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and conversion\n",
    "\n",
    "This is a couple step process:\n",
    "\n",
    "1. Load the raw data using the Open EPhys Python data loaders. \n",
    "    \n",
    "    *Do not change the directory structure, since the functions look for specific directories (ie Raw_Data) and get current and distance information from the subdirectory names*\n",
    "\n",
    "2. Remove stimulation artifacts using the ERAASR algorithm (see **Methods** for more information), then filters with a 300-6000 hz BPF\n",
    "\n",
    "3. Use Kilosort4 to extract spike times\n",
    "\n",
    "\n",
    "For each of these steps, the code will create a new numpy file for each recording in the \"Processed_Data\" directory. The next step will look for the appropriate numpy file\n",
    "\n",
    "You will need to change the value of ``` base_dir ``` to point to where you downloaded the data. Other than that, you should not need to change any of the code\n",
    "\n",
    "\n",
    "By default, this code looks to see if the processed data already exists and does not reproduce the processed data. If you want it to reprocess the data, set the \"reconvert\" flag to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find Raw_Data folder. Are you sure you downloaded the directory correctly?\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Reader needs file name or open file-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\\\\\striatum\\\\shared\\\\BrainPatch\\\\Published_Data\\\\64-4shank-poly-brainpatch-chanMap.mat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     14\u001b[0m kilosort_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobe_name\u001b[39m\u001b[38;5;124m'\u001b[39m:probe_path,\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_chan_bin\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m64\u001b[39m, \u001b[38;5;66;03m# 64 channel probe\u001b[39;00m\n\u001b[0;32m     16\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest_chans\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m# the electrodes are far enough apart we shouldn't get shared signal\u001b[39;00m\n\u001b[0;32m     17\u001b[0m             }\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# run through bulk_preprocess.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mephys_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbulk_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRaw_Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mprocessed_data_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProcessed_Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mprobe_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprobe_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkilosort_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkilosort_settings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin\\Documents\\git\\Work_Tools\\BrainPatch_analysis\\ephys_utils.py:402\u001b[0m, in \u001b[0;36mbulk_preprocess\u001b[1;34m(raw_data_dir, processed_data_dir, reconvert, probe_path, kilosort_settings)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# probe map\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m probe \u001b[38;5;241m=\u001b[39m \u001b[43mkilosort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_probe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobe_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m probe_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# default kilosort settings if it's empty\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kilosort_settings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\io.py:88\u001b[0m, in \u001b[0;36mload_probe\u001b[1;34m(probe_path)\u001b[0m\n\u001b[0;32m     85\u001b[0m     probe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkcoords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(probe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkcoords\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m probe_path\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     mat \u001b[38;5;241m=\u001b[39m \u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobe_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     connected \u001b[38;5;241m=\u001b[39m mat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnected\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     90\u001b[0m     probe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxcoords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)[connected]\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:47\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_like, mode), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Reader needs file name or open file-like object"
     ]
    }
   ],
   "source": [
    "# local copy of the data\n",
    "base_dir = 'Z:/BrainPatch/Published_Data' # this is where you downloaded the data\n",
    "base_dir = Path(base_dir) if not isinstance(base_dir, os.PathLike) else base_dir # turn it into a Path object\n",
    "\n",
    "# do we want to reprocess the data if it already exists?\n",
    "reconvert = False # change if you want to run through the whole process\n",
    "\n",
    "\n",
    "# if the structure of \"base_dir is correct\"\n",
    "if ephys_utils.base_dir_structure_check(base_dir) == 1:\n",
    "\n",
    "    # probe and settings for Kilosort4\n",
    "    probe_path = base_dir / Path(\"64-4shank-poly-brainpatch-chanMap.mat\")\n",
    "    kilosort_settings = {'probe_name':probe_path,\n",
    "                'n_chan_bin':64, # 64 channel probe\n",
    "                'nearest_chans':0, # the electrodes are far enough apart we shouldn't get shared signal\n",
    "                }\n",
    "\n",
    "    # run through bulk_preprocess.\n",
    "    ephys_utils.bulk_preprocess(raw_data_dir = base_dir / Path('Raw_Data'),\n",
    "                                processed_data_dir= base_dir / Path('Processed_Data'),\n",
    "                                probe_path= probe_path,\n",
    "                                kilosort_settings=kilosort_settings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.exists(base_dir / Path('Raw_Data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3\n",
    "\n",
    "Example waveforms from the kilosort data. One from each channel. \n",
    "\n",
    "The example waveforms are from units that have a pre-stimulation mean firing rate of at least 0.5Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a per-channel index of the waveform from the FR dataframe\n",
    "# loaded from a csv\n",
    "wf_mapping = pd.read_csv()\n",
    "wf_mapping = {\n",
    "    41: 620, # could also be 211, 481, or 340\n",
    "    34: 208, # or 745\n",
    "    56: 113, # or 487\n",
    "    62: 751,\n",
    "    0: 350,\n",
    "    6: 220,\n",
    "    23: 120,\n",
    "    28: 358,\n",
    "    32: 232,\n",
    "    45: 20,\n",
    "    58: 363,\n",
    "    54: 236,\n",
    "    8: 370,\n",
    "    19: 243,\n",
    "    36:126, \n",
    "    37: 33,\n",
    "    63: 255,\n",
    "    1: 387,\n",
    "    2: 388,\n",
    "    26: None,\n",
    "    38: 524,\n",
    "    49: 786,\n",
    "    52: 526,\n",
    "    10: 149,\n",
    "    3: 538,\n",
    "    24: None,\n",
    "    35: 286,\n",
    "    39: 412,\n",
    "    57: 159,\n",
    "    49: 556,\n",
    "    5: 818, \n",
    "    25: None,\n",
    "    42: 569,\n",
    "    51: 689,\n",
    "    44: 570,\n",
    "    50: 306,\n",
    "    18: 76, # only exists in one recording, might toss\n",
    "    20: None,\n",
    "    43: 312,\n",
    "    33: 710,\n",
    "    46: 83,\n",
    "    9: None,\n",
    "    31: 192,\n",
    "    47: 457,\n",
    "    53: 195,\n",
    "    40: 464,\n",
    "    48: 868,\n",
    "    14: 739,\n",
    "    11: 335,\n",
    "    17: 338,\n",
    "    60: 383,\n",
    "    27: None,\n",
    "    61: 400,\n",
    "    29: None,\n",
    "    12: 576,\n",
    "    4: 239,\n",
    "    30: 373,\n",
    "    7: 422,\n",
    "    16: None,\n",
    "    15: None,\n",
    "    22: 736,\n",
    "}\n",
    "\n",
    "# Waveforms, laid out according to the probe mapping from NeuroNexus\n",
    "probe_grid = plt.GridSpec(16,4, wspace=.5, hspace=.7)\n",
    "\n",
    "fig_probe = plt.figure()\n",
    "\n",
    "ax_probe = dict()\n",
    "for i_channel, (channel,waveform) in enumerate(wf_dict.items()):\n",
    "    row = int(probe['yc'][channel]/50)\n",
    "    col = int(probe['kcoords'][channel]) - 1\n",
    "\n",
    "    ax_probe[channel] = fig_probe.add_subplot(probe_grid[row,col])\n",
    "    ax_probe[channel].plot(waveform)\n",
    "    ax_probe[channel].set_title(f'channel')\n",
    "    print(channel)\n",
    "\n",
    "    for spine in ax_probe[channel].spines:\n",
    "        ax_probe[channel].spines[spine].set_visible(False)\n",
    "\n",
    "    ax_probe[channel].set_xticks([])\n",
    "    ax_probe[channel].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3g\n",
    "firing rates vs current at different depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dirs = ['Z://BrainPatch//20241002//lateral//',\n",
    "             'Z://BrainPatch//20240925//',\n",
    "             'Z:BrainPatch//20240821']\n",
    "\n",
    "# all 2ms stimulations at 400 um in the base_dirs\n",
    "directories = [os.path.join(base_dir,directory) for base_dir in base_dirs for directory in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir,directory)) and '2ms' in directory and '400um' in directory]\n",
    "\n",
    "# probe map\n",
    "probe_name = \"Z:\\\\BrainPatch\\\\20241002\\\\64-4shank-poly-brainpatch-chanMap.mat\"\n",
    "\n",
    "# settings for kilosort\n",
    "settings = {'probe_name':probe_name,\n",
    "            'n_chan_bin':64,\n",
    "            'nearest_chans':1}\n",
    "\n",
    "\n",
    "# have to define this somewhere -- probably in the utils\n",
    "FR_df = gimme_a_FR_df(directories, probe_name, settings) \n",
    "\n",
    "\n",
    "\n",
    "fig_amp_scatter, ax_amp_scatter = plt.subplots(nrows = 4, ncols=2, sharex=True, sharey=True)\n",
    "fig_amp_line, ax_amp_line = plt.subplots(nrows = 4, ncols = 2, sharex=True, sharey=True)\n",
    "\n",
    "fig_amp_line.set_size_inches(6, 10)\n",
    "\n",
    "fid_response = open('Z:\\\\BrainPatch\\\\Current_vs_responses_stim_responses.csv','w')\n",
    "fid_means = open('Z:\\\\BrainPatch\\\\Current_vs_responses_prestim_means.csv','w')\n",
    "\n",
    "for i_current, current in enumerate(FR_df['current'].unique()):\n",
    "    # first 2ms firing rate -- scatter\n",
    "    FR_df.loc[FR_df['current'].eq(current)].plot.scatter(ax = ax_amp_scatter[i_current, 1], x = 'poststim_first', y = 'depth', s=2)\n",
    "    ax_amp_scatter[i_current,1].set_title(f'{current} mA stimulation responses')\n",
    "    \n",
    "    # line plot of response mean and std \n",
    "    summary = FR_df.loc[FR_df['current'].eq(current)].groupby('depth')['poststim_first'].agg(['mean','std']) #.plot(ax = ax_amp_line[i_current, 1], x = 'mean', y='depth', xerr = 'std')\n",
    "    ax_amp_line[i_current, 1].plot(summary['mean'], summary.index)\n",
    "    ax_amp_line[i_current, 1].fill_betweenx(summary.index, np.maximum(summary['mean'] - summary['std'],0), summary['mean'] + summary['std'], alpha=0.2)\n",
    "    ax_amp_line[i_current,1].set_title(f'{current} mA stimulation responses')\n",
    "\n",
    "    # stim to csv\n",
    "    summary['current'] = current\n",
    "    summary.to_csv(fid_response, header=(i_current==0))\n",
    "\n",
    "\n",
    "    # pre-stimulation mean\n",
    "    FR_df.loc[FR_df['current'].eq(current)].plot.scatter(ax = ax_amp_scatter[i_current, 0], x = 'prestim_mean', y = 'depth', s=2)\n",
    "    ax_amp_scatter[i_current,0].set_title(f'{current} mA pre-stimulation means')\n",
    "    \n",
    "    # line plot of mean and std\n",
    "    summary = FR_df.loc[FR_df['current'].eq(current)].groupby('depth')['prestim_mean'].agg(['mean','std']) #.plot(ax = ax_amp_line[i_current, 1], x = 'mean', y='depth', xerr = 'std')\n",
    "    ax_amp_line[i_current, 0].plot(summary['mean'], summary.index)\n",
    "    ax_amp_line[i_current, 0].fill_betweenx(summary.index, np.maximum(summary['mean'] - summary['std'],0), summary['mean'] + summary['std'], alpha=0.2)\n",
    "    ax_amp_line[i_current,0].set_title(f'{current} mA pre-stimulation means')\n",
    "    ax_amp_line[i_current,0].set_ylabel('Depth $\\mu$m')\n",
    "\n",
    "    # stim to csv\n",
    "    summary['current'] = current\n",
    "    summary.to_csv(fid_means, header=(i_current==0))\n",
    "    \n",
    "    # remove the spines from the axes\n",
    "    for spine in ax_amp_line[i_current,0].spines:\n",
    "        ax_amp_line[i_current,0].spines[spine].set_visible(False)\n",
    "        ax_amp_line[i_current,1].spines[spine].set_visible(False)\n",
    "\n",
    "fid_means.close()\n",
    "fid_response.close()\n",
    "\n",
    "fig_amp_line.savefig('Z://BrainPatch//current_vs_response.svg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Figure \n",
    "This figure shows some of the preprocessing steps, and goes into the LFP responses at different distances.\n",
    "\n",
    "\n",
    "First, let's look at the artifacts that are produced solely by the LED/current source. This dataset is recorded from a mouse without ChrimsonR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'Z:\\\\BrainPatch\\\\Raw_Data\\\\Wildtype_Artifacts'\n",
    "\n",
    "fig_raw,ax_raw = plt.subplots()\n",
    "fig_eraasr,ax_eraasr = plt.subplots()\n",
    "\n",
    "# for the artifact\n",
    "csv_file_raw = open('Z:\\\\BrainPatch\\\\Figures\\\\Supplemental\\\\Artifacts_raw.csv', 'w')\n",
    "csv_writer_raw = csv.writer(csv_file_raw)\n",
    "csv_writer_raw.writerow(['current','','trace'])\n",
    "\n",
    "# errasr'd artifact\n",
    "csv_file_eraasr = open('Z:\\\\BrainPatch\\\\Figures\\\\Supplemental\\\\Artifacts_eraasr.csv', 'w')\n",
    "csv_writer_eraasr = csv.writer(csv_file_eraasr)\n",
    "csv_writer_eraasr.writerow(['current','','trace'])\n",
    "\n",
    "for i_directory,directory in enumerate([dd for dd in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir,dd))]):\n",
    "    # distance information from the directory name\n",
    "    distance = re.search('(\\d{4})um', directory)[0]\n",
    "\n",
    "    # load the data - previously loaded if available\n",
    "    sig, timestamps, stim, stim_ts = openephys_utils.open_sig_stims(os.path.join(base_dir,directory))\n",
    "    sig_eraasr = openephys_utils.ERAASR(sig, stim, save=False)\n",
    "\n",
    "    # plot the mean waveform for each stimulation distance\n",
    "    openephys_utils.plot_mean_LFP(sig, stim, channel = 45, pre_stim=1, ax=ax_raw, show_stim=i_directory==0, label=distance, len_ms = 10, align_stim=False)\n",
    "    openephys_utils.plot_mean_LFP(sig_eraasr, stim, channel = 45, pre_stim=1, ax=ax_eraasr, show_stim=i_directory==0, label=distance, len_ms = 10, align_stim=False)\n",
    "\n",
    "\n",
    "# get the traces and put them into the csv file\n",
    "for child in ax_raw.lines:\n",
    "    data = np.stack(child.get_data())\n",
    "    csv_writer_raw.writerow(data)\n",
    "    \n",
    "for child in ax_eraasr.lines:\n",
    "    data = np.stack(child.get_data())\n",
    "    csv_writer_eraasr.writerow(data)\n",
    "\n",
    "\n",
    "# clean up the plot\n",
    "ax_raw.legend() # add a plot\n",
    "ax_eraasr.legend()\n",
    "for spine in ax_raw.spines: # turn off the box around the axis\n",
    "    ax_raw.spines[spine].set_visible(False) \n",
    "    ax_eraasr.spines[spine].set_visible(False) \n",
    "\n",
    "ax_raw.set_xlabel('Time (ms)')\n",
    "ax_raw.set_ylabel('Voltage (mV)')\n",
    "ax_eraasr.set_xlabel('Time (ms)')\n",
    "ax_eraasr.set_ylabel('Voltage (mV)')\n",
    "\n",
    "ax_eraasr.set_ylim(ax_raw.get_ylim())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the LFP from a recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 recordings in Z:/BrainPatch/Raw_Data/20241002\n",
      "4 unique current values and 5 unique distances\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4fa13515624ec88e7ef3e16be3c148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20_400', '20_400', '15_400', '15_400', '10_400', '10_400', '5_400', '15_400', '5_600', '15_600', '20_600', '20_900', '15_900', '10_900', '5_900', '5_1200', '10_1200', '15_1200', '20_1200', '20_1500', '15_1500', '10_1500']\n"
     ]
    }
   ],
   "source": [
    "# plot it with openephys_utils\n",
    "openephys_utils.LFP_stim_bulk('Z:/BrainPatch/Raw_Data/20241002')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import collections\n",
    "\n",
    "# csv\n",
    "csv_file_min = open('Z:\\\\BrainPatch\\\\Figures\\\\Supplemental\\\\mean_response_min_time.csv', 'w')\n",
    "csv_writer_min = csv.writer(csv_file_min)\n",
    "csv_writer_min.writerow(['current','time', 'distance'])\n",
    "\n",
    "fig = plt.gcf()\n",
    "for ax in fig.get_axes():\n",
    "    ax_child = ax.get_children()\n",
    "    for child in ax_child:\n",
    "        if type(child) == collections.PathCollection:\n",
    "            offs = child.get_offsets().data\n",
    "            offs = np.append(offs, np.ones((offs.shape[0],1))*int(ax.get_title().strip(' mm')), axis=1)\n",
    "            csv_writer_min.writerows(offs)\n",
    "\n",
    "\n",
    "csv_file_min.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from matplotlib import collections\n",
    "\n",
    "# csv\n",
    "csv_file_min = open('Z:\\\\BrainPatch\\\\Figures\\\\Supplemental\\\\mean_response_min_depth.csv', 'w')\n",
    "csv_writer_min = csv.writer(csv_file_min)\n",
    "csv_writer_min.writerow(['magnitude','distance', 'current'])\n",
    "\n",
    "fig = plt.gcf()\n",
    "for ax in fig.get_axes():\n",
    "    ax_child = ax.get_children()\n",
    "    for child in ax_child:\n",
    "        if type(child) == collections.PathCollection:\n",
    "            offs = child.get_offsets().data\n",
    "            offs = np.append(offs, np.ones((offs.shape[0],1))*int(ax.get_title().strip(' mA')), axis=1)\n",
    "            csv_writer_min.writerows(offs)\n",
    "\n",
    "\n",
    "csv_file_min.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
