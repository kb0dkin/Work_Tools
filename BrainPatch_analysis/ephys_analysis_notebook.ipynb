{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainPatch in-vivo electrophysiology code\n",
    "All of the following code is for analysis of the in-vivo recordings \n",
    "\n",
    "## Import python packages\n",
    "All of these should be working properly if you've used the provided conda environment file. However, if there are any versioning issues just reach out to me on github @kb0dkin and we'll get it sorted out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\__init__.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    }
   ],
   "source": [
    "import ephys_utils # loading and basic processing code.\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Polygon # this is a nice way to show error bars and standard deviations\n",
    "\n",
    "# data analysis standards\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import os, glob, re\n",
    "from pathlib import Path\n",
    "\n",
    "# csv writing\n",
    "import csv\n",
    "\n",
    "# tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# open the plots with QT\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and conversion\n",
    "\n",
    "This is a couple step process:\n",
    "\n",
    "1. Load the raw data using the Open EPhys Python data loaders. \n",
    "    \n",
    "    *Do not change the directory structure, since the functions look for specific directories (ie Raw_Data) and get current and distance information from the subdirectory names*\n",
    "\n",
    "2. Remove stimulation artifacts using the ERAASR algorithm (see **Methods** for more information), then filters with a 300-6000 hz BPF\n",
    "\n",
    "3. Use Kilosort4 to extract spike times\n",
    "\n",
    "\n",
    "For each of these steps, the code will create a new numpy file for each recording in the \"Processed_Data\" directory. The next step will look for the appropriate numpy file\n",
    "\n",
    "You will need to change the value of ``` base_dir ``` to point to where you downloaded the data. Other than that, you should not need to change any of the code\n",
    "\n",
    "\n",
    "**NOTE**\n",
    "This is a fairly long process! By default, this code looks to see if the processed data already exists and does not reproduce the processed data. If you want it to reprocess the data, set the \"reconvert\" flag to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff35f5ba798433d9324189a73c849f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[bulk_preprocess] processing files:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bulk process] loading raw data            *\n",
      "[bulk process] cleaning artifacts            *\n",
      "[bulk process] filtering            *\n",
      "[bulk process] loading raw data            *\n",
      "[bulk process] cleaning artifacts            *\n",
      "[bulk process] filtering            *\n",
      "[bulk process] loading raw data            *\n",
      "[bulk process] cleaning artifacts            *\n",
      "[bulk process] filtering            *\n",
      "[bulk process] kilosort                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kilosort.run_kilosort: Kilosort version 4.0.16\n",
      "kilosort.run_kilosort: Sorting Z:\\BrainPatch\\Published_Data\\Processed_Data\\Crimson__2024-08-21_13-44-07__10mA_2ms_400um\\sig_filter.npy\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "kilosort.run_kilosort: Using GPU for PyTorch computations. Specify `device` to change this.\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Computing preprocessing variables.\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "kilosort.run_kilosort: N samples: 2630400\n",
      "kilosort.run_kilosort: N seconds: 87.68\n",
      "kilosort.run_kilosort: N batches: 44\n",
      "kilosort.run_kilosort: Preprocessing filters computed in  2.88s; total  2.88s\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Computing drift correction.\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "kilosort.spikedetect: Re-computing universal templates from data.\n",
      "e:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "kilosort.run_kilosort: Encountered error in `run_kilosort`:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\run_kilosort.py\", line 205, in run_kilosort\n",
      "    ops, bfile, st0 = compute_drift_correction(\n",
      "  File \"e:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\run_kilosort.py\", line 520, in compute_drift_correction\n",
      "    ops, st = datashift.run(ops, bfile, device=device, progress_bar=progress_bar,\n",
      "  File \"e:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\datashift.py\", line 198, in run\n",
      "    st, _, ops  = spikedetect.run(\n",
      "  File \"e:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\spikedetect.py\", line 227, in run\n",
      "    igood = ds[0,:] <= ops['max_channel_distance']**2\n",
      "IndexError: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     14\u001b[0m kilosort_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobe_name\u001b[39m\u001b[38;5;124m'\u001b[39m:probe_path,\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_chan_bin\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m64\u001b[39m, \u001b[38;5;66;03m# 64 channel probe\u001b[39;00m\n\u001b[0;32m     16\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest_chans\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m# the electrodes are far enough apart we shouldn't get shared signal\u001b[39;00m\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrift_correction\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     18\u001b[0m             }\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# run through bulk_preprocess.\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mephys_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbulk_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRaw_Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mprocessed_data_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProcessed_Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mprobe_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprobe_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkilosort_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkilosort_settings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin\\Documents\\git\\Work_Tools\\BrainPatch_analysis\\ephys_utils.py:457\u001b[0m, in \u001b[0;36mbulk_preprocess\u001b[1;34m(raw_data_dir, processed_data_dir, reconvert, probe_path, kilosort_settings)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mexists(kilosort_dir):\n\u001b[0;32m    456\u001b[0m     kilosort_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filt_path \u001b[38;5;66;03m# had issues with it loading the data, so I give it the data both from a file and from an array\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     kilosort\u001b[38;5;241m.\u001b[39mrun_kilosort(kilosort_settings, file_object\u001b[38;5;241m=\u001b[39msig_filter\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32), data_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, results_dir\u001b[38;5;241m=\u001b[39mkilosort_dir)\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\run_kilosort.py:205\u001b[0m, in \u001b[0;36mrun_kilosort\u001b[1;34m(settings, probe, probe_name, filename, data_dir, file_object, results_dir, data_dtype, do_CAR, invert_sign, device, progress_bar, save_extra_vars, clear_cache, save_preprocessed_copy, bad_channels)\u001b[0m\n\u001b[0;32m    203\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    204\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m ops, bfile, st0 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_drift_correction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtic0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtic0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclear_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Check scale of data for log file\u001b[39;00m\n\u001b[0;32m    211\u001b[0m b1 \u001b[38;5;241m=\u001b[39m bfile\u001b[38;5;241m.\u001b[39mpadded_batch_to_torch(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\run_kilosort.py:520\u001b[0m, in \u001b[0;36mcompute_drift_correction\u001b[1;34m(ops, device, tic0, progress_bar, file_object, clear_cache)\u001b[0m\n\u001b[0;32m    510\u001b[0m whiten_mat \u001b[38;5;241m=\u001b[39m ops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhiten_mat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    512\u001b[0m bfile \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBinaryFiltered(\n\u001b[0;32m    513\u001b[0m     ops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m], n_chan_bin, fs, NT, nt, twav_min, chan_map, \n\u001b[0;32m    514\u001b[0m     hp_filter\u001b[38;5;241m=\u001b[39mhp_filter, whiten_mat\u001b[38;5;241m=\u001b[39mwhiten_mat, device\u001b[38;5;241m=\u001b[39mdevice, do_CAR\u001b[38;5;241m=\u001b[39mdo_CAR,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m     file_object\u001b[38;5;241m=\u001b[39mfile_object\n\u001b[0;32m    518\u001b[0m     )\n\u001b[1;32m--> 520\u001b[0m ops, st \u001b[38;5;241m=\u001b[39m \u001b[43mdatashift\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mclear_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclear_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m bfile\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    523\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrift computed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mtic\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms; \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m    524\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mtic0\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\datashift.py:198\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(ops, bfile, device, progress_bar, clear_cache)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ops, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# the first step is to extract all spikes using the universal templates\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m st, _, ops  \u001b[38;5;241m=\u001b[39m \u001b[43mspikedetect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclear_cache\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# spikes are binned by amplitude and y-position to construct a \"fingerprint\" for each batch\u001b[39;00m\n\u001b[0;32m    204\u001b[0m F, ysamp \u001b[38;5;241m=\u001b[39m bin_spikes(ops, st)\n",
      "File \u001b[1;32me:\\Kevin\\Anaconda\\conda_envs\\brainpatch_analysis\\lib\\site-packages\\kilosort\\spikedetect.py:227\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(ops, bfile, device, progress_bar, clear_cache)\u001b[0m\n\u001b[0;32m    223\u001b[0m iC, ds \u001b[38;5;241m=\u001b[39m nearest_chans(ys, yc, xs, xc, nC, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Don't use templates that are too far away from nearest channel\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# (use square of max distance since ds are squared distances)\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m igood \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m ops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_channel_distance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    228\u001b[0m iC \u001b[38;5;241m=\u001b[39m iC[:,igood]\n\u001b[0;32m    229\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds[:,igood]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# local copy of the data\n",
    "base_dir = 'Z:/BrainPatch/Published_Data' # this is where you downloaded the data\n",
    "base_dir = Path(base_dir) if not isinstance(base_dir, os.PathLike) else base_dir # turn it into a Path object\n",
    "\n",
    "# do we want to reprocess the data if it already exists?\n",
    "reconvert = False # change if you want to run through the whole process\n",
    "\n",
    "\n",
    "# if the structure of \"base_dir is correct\"\n",
    "if ephys_utils.base_dir_structure_check(base_dir) == 1:\n",
    "\n",
    "    # probe and settings for Kilosort4\n",
    "    probe_path = base_dir / Path(\"64-4shank-poly-brainpatch-chanMap.mat\")\n",
    "    kilosort_settings = {'probe_name':probe_path,\n",
    "                'n_chan_bin':64, # 64 channel probe\n",
    "                'nearest_chans':0, # the electrodes are far enough apart we shouldn't get shared signal\n",
    "                'drift_correction':False\n",
    "                }\n",
    "\n",
    "    # run through bulk_preprocess.\n",
    "    ephys_utils.bulk_preprocess(raw_data_dir = base_dir / Path('Raw_Data'),\n",
    "                                processed_data_dir= base_dir / Path('Processed_Data'),\n",
    "                                probe_path= probe_path,\n",
    "                                kilosort_settings=kilosort_settings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next calculate firing rates, and find the differences between pre-stimulation firing rates and post-stimulation firing rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_df = ephys_utils.firingRate_dataframe(directories=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3\n",
    "\n",
    "Example waveforms from the kilosort data. One from each channel. \n",
    "\n",
    "The example waveforms are from units that have a pre-stimulation mean firing rate of at least 0.5Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a per-channel index of the waveform from the FR dataframe\n",
    "# loaded from a csv\n",
    "wf_mapping = pd.read_csv()\n",
    "wf_mapping = {\n",
    "    41: 620, # could also be 211, 481, or 340\n",
    "    34: 208, # or 745\n",
    "    56: 113, # or 487\n",
    "    62: 751,\n",
    "    0: 350,\n",
    "    6: 220,\n",
    "    23: 120,\n",
    "    28: 358,\n",
    "    32: 232,\n",
    "    45: 20,\n",
    "    58: 363,\n",
    "    54: 236,\n",
    "    8: 370,\n",
    "    19: 243,\n",
    "    36:126, \n",
    "    37: 33,\n",
    "    63: 255,\n",
    "    1: 387,\n",
    "    2: 388,\n",
    "    26: None,\n",
    "    38: 524,\n",
    "    49: 786,\n",
    "    52: 526,\n",
    "    10: 149,\n",
    "    3: 538,\n",
    "    24: None,\n",
    "    35: 286,\n",
    "    39: 412,\n",
    "    57: 159,\n",
    "    49: 556,\n",
    "    5: 818, \n",
    "    25: None,\n",
    "    42: 569,\n",
    "    51: 689,\n",
    "    44: 570,\n",
    "    50: 306,\n",
    "    18: 76, # only exists in one recording, might toss\n",
    "    20: None,\n",
    "    43: 312,\n",
    "    33: 710,\n",
    "    46: 83,\n",
    "    9: None,\n",
    "    31: 192,\n",
    "    47: 457,\n",
    "    53: 195,\n",
    "    40: 464,\n",
    "    48: 868,\n",
    "    14: 739,\n",
    "    11: 335,\n",
    "    17: 338,\n",
    "    60: 383,\n",
    "    27: None,\n",
    "    61: 400,\n",
    "    29: None,\n",
    "    12: 576,\n",
    "    4: 239,\n",
    "    30: 373,\n",
    "    7: 422,\n",
    "    16: None,\n",
    "    15: None,\n",
    "    22: 736,\n",
    "}\n",
    "\n",
    "# Waveforms, laid out according to the probe mapping from NeuroNexus\n",
    "probe_grid = plt.GridSpec(16,4, wspace=.5, hspace=.7)\n",
    "\n",
    "fig_probe = plt.figure()\n",
    "\n",
    "ax_probe = dict()\n",
    "for i_channel, (channel,waveform) in enumerate(wf_dict.items()):\n",
    "    row = int(probe['yc'][channel]/50)\n",
    "    col = int(probe['kcoords'][channel]) - 1\n",
    "\n",
    "    ax_probe[channel] = fig_probe.add_subplot(probe_grid[row,col])\n",
    "    ax_probe[channel].plot(waveform)\n",
    "    ax_probe[channel].set_title(f'channel')\n",
    "    print(channel)\n",
    "\n",
    "    for spine in ax_probe[channel].spines:\n",
    "        ax_probe[channel].spines[spine].set_visible(False)\n",
    "\n",
    "    ax_probe[channel].set_xticks([])\n",
    "    ax_probe[channel].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3g\n",
    "firing rates vs current at different depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dirs = ['Z://BrainPatch//20241002//lateral//',\n",
    "             'Z://BrainPatch//20240925//',\n",
    "             'Z:BrainPatch//20240821']\n",
    "\n",
    "# all 2ms stimulations at 400 um in the base_dirs\n",
    "directories = [os.path.join(base_dir,directory) for base_dir in base_dirs for directory in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir,directory)) and '2ms' in directory and '400um' in directory]\n",
    "\n",
    "# probe map\n",
    "probe_name = \"Z:\\\\BrainPatch\\\\20241002\\\\64-4shank-poly-brainpatch-chanMap.mat\"\n",
    "\n",
    "# settings for kilosort\n",
    "settings = {'probe_name':probe_name,\n",
    "            'n_chan_bin':64,\n",
    "            'nearest_chans':1}\n",
    "\n",
    "\n",
    "# have to define this somewhere -- probably in the utils\n",
    "FR_df = gimme_a_FR_df(directories, probe_name, settings) \n",
    "\n",
    "\n",
    "\n",
    "fig_amp_scatter, ax_amp_scatter = plt.subplots(nrows = 4, ncols=2, sharex=True, sharey=True)\n",
    "fig_amp_line, ax_amp_line = plt.subplots(nrows = 4, ncols = 2, sharex=True, sharey=True)\n",
    "\n",
    "fig_amp_line.set_size_inches(6, 10)\n",
    "\n",
    "fid_response = open('Z:\\\\BrainPatch\\\\Current_vs_responses_stim_responses.csv','w')\n",
    "fid_means = open('Z:\\\\BrainPatch\\\\Current_vs_responses_prestim_means.csv','w')\n",
    "\n",
    "for i_current, current in enumerate(FR_df['current'].unique()):\n",
    "    # first 2ms firing rate -- scatter\n",
    "    FR_df.loc[FR_df['current'].eq(current)].plot.scatter(ax = ax_amp_scatter[i_current, 1], x = 'poststim_first', y = 'depth', s=2)\n",
    "    ax_amp_scatter[i_current,1].set_title(f'{current} mA stimulation responses')\n",
    "    \n",
    "    # line plot of response mean and std \n",
    "    summary = FR_df.loc[FR_df['current'].eq(current)].groupby('depth')['poststim_first'].agg(['mean','std']) #.plot(ax = ax_amp_line[i_current, 1], x = 'mean', y='depth', xerr = 'std')\n",
    "    ax_amp_line[i_current, 1].plot(summary['mean'], summary.index)\n",
    "    ax_amp_line[i_current, 1].fill_betweenx(summary.index, np.maximum(summary['mean'] - summary['std'],0), summary['mean'] + summary['std'], alpha=0.2)\n",
    "    ax_amp_line[i_current,1].set_title(f'{current} mA stimulation responses')\n",
    "\n",
    "    # stim to csv\n",
    "    summary['current'] = current\n",
    "    summary.to_csv(fid_response, header=(i_current==0))\n",
    "\n",
    "\n",
    "    # pre-stimulation mean\n",
    "    FR_df.loc[FR_df['current'].eq(current)].plot.scatter(ax = ax_amp_scatter[i_current, 0], x = 'prestim_mean', y = 'depth', s=2)\n",
    "    ax_amp_scatter[i_current,0].set_title(f'{current} mA pre-stimulation means')\n",
    "    \n",
    "    # line plot of mean and std\n",
    "    summary = FR_df.loc[FR_df['current'].eq(current)].groupby('depth')['prestim_mean'].agg(['mean','std']) #.plot(ax = ax_amp_line[i_current, 1], x = 'mean', y='depth', xerr = 'std')\n",
    "    ax_amp_line[i_current, 0].plot(summary['mean'], summary.index)\n",
    "    ax_amp_line[i_current, 0].fill_betweenx(summary.index, np.maximum(summary['mean'] - summary['std'],0), summary['mean'] + summary['std'], alpha=0.2)\n",
    "    ax_amp_line[i_current,0].set_title(f'{current} mA pre-stimulation means')\n",
    "    ax_amp_line[i_current,0].set_ylabel('Depth $\\mu$m')\n",
    "\n",
    "    # stim to csv\n",
    "    summary['current'] = current\n",
    "    summary.to_csv(fid_means, header=(i_current==0))\n",
    "    \n",
    "    # remove the spines from the axes\n",
    "    for spine in ax_amp_line[i_current,0].spines:\n",
    "        ax_amp_line[i_current,0].spines[spine].set_visible(False)\n",
    "        ax_amp_line[i_current,1].spines[spine].set_visible(False)\n",
    "\n",
    "fid_means.close()\n",
    "fid_response.close()\n",
    "\n",
    "fig_amp_line.savefig('Z://BrainPatch//current_vs_response.svg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Figure \n",
    "This figure shows some of the preprocessing steps, and goes into the LFP responses at different distances.\n",
    "\n",
    "\n",
    "First, let's look at the artifacts that are produced solely by the LED/current source. This dataset is recorded from a mouse without ChrimsonR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'Z:\\\\BrainPatch\\\\Raw_Data\\\\Wildtype_Artifacts'\n",
    "\n",
    "fig_raw,ax_raw = plt.subplots()\n",
    "fig_eraasr,ax_eraasr = plt.subplots()\n",
    "\n",
    "# for the artifact\n",
    "csv_file_raw = open('Z:\\\\BrainPatch\\\\Figures\\\\Supplemental\\\\Artifacts_raw.csv', 'w')\n",
    "csv_writer_raw = csv.writer(csv_file_raw)\n",
    "csv_writer_raw.writerow(['current','','trace'])\n",
    "\n",
    "# errasr'd artifact\n",
    "csv_file_eraasr = open('Z:\\\\BrainPatch\\\\Figures\\\\Supplemental\\\\Artifacts_eraasr.csv', 'w')\n",
    "csv_writer_eraasr = csv.writer(csv_file_eraasr)\n",
    "csv_writer_eraasr.writerow(['current','','trace'])\n",
    "\n",
    "for i_directory,directory in enumerate([dd for dd in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir,dd))]):\n",
    "    # distance information from the directory name\n",
    "    distance = re.search('(\\d{4})um', directory)[0]\n",
    "\n",
    "    # load the data - previously loaded if available\n",
    "    sig, timestamps, stim, stim_ts = openephys_utils.open_sig_stims(os.path.join(base_dir,directory))\n",
    "    sig_eraasr = openephys_utils.ERAASR(sig, stim, save=False)\n",
    "\n",
    "    # plot the mean waveform for each stimulation distance\n",
    "    openephys_utils.plot_mean_LFP(sig, stim, channel = 45, pre_stim=1, ax=ax_raw, show_stim=i_directory==0, label=distance, len_ms = 10, align_stim=False)\n",
    "    openephys_utils.plot_mean_LFP(sig_eraasr, stim, channel = 45, pre_stim=1, ax=ax_eraasr, show_stim=i_directory==0, label=distance, len_ms = 10, align_stim=False)\n",
    "\n",
    "\n",
    "# get the traces and put them into the csv file\n",
    "for child in ax_raw.lines:\n",
    "    data = np.stack(child.get_data())\n",
    "    csv_writer_raw.writerow(data)\n",
    "    \n",
    "for child in ax_eraasr.lines:\n",
    "    data = np.stack(child.get_data())\n",
    "    csv_writer_eraasr.writerow(data)\n",
    "\n",
    "\n",
    "# clean up the plot\n",
    "ax_raw.legend() # add a plot\n",
    "ax_eraasr.legend()\n",
    "for spine in ax_raw.spines: # turn off the box around the axis\n",
    "    ax_raw.spines[spine].set_visible(False) \n",
    "    ax_eraasr.spines[spine].set_visible(False) \n",
    "\n",
    "ax_raw.set_xlabel('Time (ms)')\n",
    "ax_raw.set_ylabel('Voltage (mV)')\n",
    "ax_eraasr.set_xlabel('Time (ms)')\n",
    "ax_eraasr.set_ylabel('Voltage (mV)')\n",
    "\n",
    "ax_eraasr.set_ylim(ax_raw.get_ylim())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the LFP from a recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it with openephys_utils\n",
    "openephys_utils.LFP_stim_bulk('Z:/BrainPatch/Raw_Data/20241002')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import collections\n",
    "\n",
    "# csv\n",
    "csv_file_min = open('Z:\\\\BrainPatch\\\\Figures\\\\Supplemental\\\\mean_response_min_time.csv', 'w')\n",
    "csv_writer_min = csv.writer(csv_file_min)\n",
    "csv_writer_min.writerow(['current','time', 'distance'])\n",
    "\n",
    "fig = plt.gcf()\n",
    "for ax in fig.get_axes():\n",
    "    ax_child = ax.get_children()\n",
    "    for child in ax_child:\n",
    "        if type(child) == collections.PathCollection:\n",
    "            offs = child.get_offsets().data\n",
    "            offs = np.append(offs, np.ones((offs.shape[0],1))*int(ax.get_title().strip(' mm')), axis=1)\n",
    "            csv_writer_min.writerows(offs)\n",
    "\n",
    "\n",
    "csv_file_min.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from matplotlib import collections\n",
    "\n",
    "# csv\n",
    "csv_file_min = open('Z:\\\\BrainPatch\\\\Figures\\\\Supplemental\\\\mean_response_min_depth.csv', 'w')\n",
    "csv_writer_min = csv.writer(csv_file_min)\n",
    "csv_writer_min.writerow(['magnitude','distance', 'current'])\n",
    "\n",
    "fig = plt.gcf()\n",
    "for ax in fig.get_axes():\n",
    "    ax_child = ax.get_children()\n",
    "    for child in ax_child:\n",
    "        if type(child) == collections.PathCollection:\n",
    "            offs = child.get_offsets().data\n",
    "            offs = np.append(offs, np.ones((offs.shape[0],1))*int(ax.get_title().strip(' mA')), axis=1)\n",
    "            csv_writer_min.writerows(offs)\n",
    "\n",
    "\n",
    "csv_file_min.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
